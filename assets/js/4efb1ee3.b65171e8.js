"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[2712],{8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>l});var s=i(6540);const t={},a=s.createContext(t);function r(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),s.createElement(a.Provider,{value:e},n.children)}},8513:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module2-digital-twin/simulating-sensors","title":"Simulating Sensors (LIDAR, Depth Cameras, IMUs)","description":"Introduction","source":"@site/docs/module2-digital-twin/simulating-sensors.md","sourceDirName":"module2-digital-twin","slug":"/module2-digital-twin/simulating-sensors","permalink":"/-physical-ai-textbook_01/docs/module2-digital-twin/simulating-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/umernasir1/physical-ai-textbook/tree/main/docs/module2-digital-twin/simulating-sensors.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Simulating Sensors (LIDAR, Depth Cameras, IMUs)"},"sidebar":"tutorialSidebar","previous":{"title":"High-fidelity Rendering and Human-Robot Interaction","permalink":"/-physical-ai-textbook_01/docs/module2-digital-twin/high-fidelity-rendering-human-robot-interaction"},"next":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","permalink":"/-physical-ai-textbook_01/docs/category/module-3-the-ai-robot-brain-nvidia-isaac"}}');var t=i(4848),a=i(8453);const r={sidebar_position:3,title:"Simulating Sensors (LIDAR, Depth Cameras, IMUs)"},l="Simulating Sensors (LIDAR, Depth Cameras, IMUs)",o={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Why Sensor Simulation Matters",id:"why-sensor-simulation-matters",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"Understanding LiDAR Technology",id:"understanding-lidar-technology",level:3},{value:"Simulating LiDAR in Gazebo",id:"simulating-lidar-in-gazebo",level:3},{value:"Processing LiDAR Data in ROS 2",id:"processing-lidar-data-in-ros-2",level:3},{value:"Visualizing LiDAR Data in RViz2",id:"visualizing-lidar-data-in-rviz2",level:3},{value:"Depth Camera Simulation (RGB-D)",id:"depth-camera-simulation-rgb-d",level:2},{value:"RGB-D Camera Technology",id:"rgb-d-camera-technology",level:3},{value:"Simulating RGB-D in Gazebo",id:"simulating-rgb-d-in-gazebo",level:3},{value:"Object Detection with RGB-D",id:"object-detection-with-rgb-d",level:3},{value:"3D Point Cloud from RGB-D",id:"3d-point-cloud-from-rgb-d",level:3},{value:"IMU Simulation (Inertial Measurement Unit)",id:"imu-simulation-inertial-measurement-unit",level:2},{value:"IMU Fundamentals",id:"imu-fundamentals",level:3},{value:"Simulating IMU in Gazebo",id:"simulating-imu-in-gazebo",level:3},{value:"Fall Detection with IMU",id:"fall-detection-with-imu",level:3},{value:"Sensor Fusion: Complementary Filter",id:"sensor-fusion-complementary-filter",level:3},{value:"Multi-Sensor Fusion",id:"multi-sensor-fusion",level:2},{value:"Kalman Filter for Sensor Integration",id:"kalman-filter-for-sensor-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Reducing Sensor Computational Load",id:"reducing-sensor-computational-load",level:3},{value:"Generating Synthetic Training Data",id:"generating-synthetic-training-data",level:2},{value:"Automated Data Collection in Simulation",id:"automated-data-collection-in-simulation",level:3},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"simulating-sensors-lidar-depth-cameras-imus",children:"Simulating Sensors (LIDAR, Depth Cameras, IMUs)"})}),"\n",(0,t.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(e.p,{children:"Autonomous humanoid robots rely on sensors to perceive their environment. Before deploying expensive hardware, simulation allows us to test sensor configurations, develop perception algorithms, and generate training data. This chapter covers the three essential sensor types for humanoid robotics:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"LiDAR"})," (Light Detection and Ranging): 3D mapping and obstacle detection"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Depth Cameras"})," (RGB-D): Close-range manipulation and object recognition"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"IMU"})," (Inertial Measurement Unit): Balance and orientation tracking"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"why-sensor-simulation-matters",children:"Why Sensor Simulation Matters"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Benefits of Simulated Sensors:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cost Savings"}),": Test configurations before purchasing hardware"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety"}),": Develop algorithms without risking real robots"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Data Generation"}),": Create labeled datasets for machine learning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sim-to-Real Transfer"}),": Algorithms trained in simulation work on real robots"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Rapid Iteration"}),": Change sensor placements instantly"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Challenges:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Noise Modeling"}),": Real sensors have noise; simulations must replicate it"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Performance"}),": Ray-tracing LiDAR with millions of points is computationally expensive"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Accuracy"}),": Physics engines approximate reality"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,t.jsx)(e.h3,{id:"understanding-lidar-technology",children:"Understanding LiDAR Technology"}),"\n",(0,t.jsx)(e.p,{children:"LiDAR measures distances by timing laser pulses reflected from surfaces. Common types:"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Type"}),(0,t.jsx)(e.th,{children:"Range"}),(0,t.jsx)(e.th,{children:"Points/Second"}),(0,t.jsx)(e.th,{children:"Use Case"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"2D LiDAR"})}),(0,t.jsx)(e.td,{children:"30m"}),(0,t.jsx)(e.td,{children:"5,000-15,000"}),(0,t.jsx)(e.td,{children:"Floor-level obstacle detection"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"3D LiDAR"})}),(0,t.jsx)(e.td,{children:"100m+"}),(0,t.jsx)(e.td,{children:"300,000-2M"}),(0,t.jsx)(e.td,{children:"Full 3D mapping (Velodyne, Ouster)"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Solid-State LiDAR"})}),(0,t.jsx)(e.td,{children:"200m"}),(0,t.jsx)(e.td,{children:"1M+"}),(0,t.jsx)(e.td,{children:"Automotive (no moving parts)"})]})]})]}),"\n",(0,t.jsxs)(e.p,{children:["For humanoid robots, ",(0,t.jsx)(e.strong,{children:"3D LiDAR"})," mounted at head height provides:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"360\xb0 environmental awareness"}),"\n",(0,t.jsx)(e.li,{children:"Stair/obstacle detection for navigation"}),"\n",(0,t.jsx)(e.li,{children:"Human detection and tracking"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"simulating-lidar-in-gazebo",children:"Simulating LiDAR in Gazebo"}),"\n",(0,t.jsxs)(e.p,{children:["Gazebo's ",(0,t.jsx)(e.code,{children:"gpu_ray"})," sensor efficiently simulates LiDAR using GPU ray-tracing."]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Example: Velodyne VLP-16 LiDAR Simulation"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Add to your robot\'s URDF/SDF file --\x3e\n<gazebo reference="lidar_link">\n  <sensor name="velodyne" type="gpu_ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>10</update_rate>\n\n    <ray>\n      \x3c!-- 16 vertical lasers (VLP-16 spec) --\x3e\n      <scan>\n        <horizontal>\n          <samples>1800</samples>  \x3c!-- 0.2\xb0 resolution --\x3e\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>  \x3c!-- -180\xb0 --\x3e\n          <max_angle>3.14159</max_angle>   \x3c!-- +180\xb0 --\x3e\n        </horizontal>\n        <vertical>\n          <samples>16</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.2617</min_angle>  \x3c!-- -15\xb0 --\x3e\n          <max_angle>0.2617</max_angle>   \x3c!-- +15\xb0 --\x3e\n        </vertical>\n      </scan>\n\n      \x3c!-- Range specifications --\x3e\n      <range>\n        <min>0.3</min>\n        <max>100.0</max>\n        <resolution>0.01</resolution>\n      </range>\n\n      \x3c!-- Simulate sensor noise --\x3e\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.02</stddev>  \x3c!-- 2cm standard deviation --\x3e\n      </noise>\n    </ray>\n\n    <plugin name="gazebo_ros_laser_controller" filename="libgazebo_ros_velodyne_laser.so">\n      <topicName>/velodyne_points</topicName>\n      <frameName>lidar_link</frameName>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,t.jsx)(e.h3,{id:"processing-lidar-data-in-ros-2",children:"Processing LiDAR Data in ROS 2"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2, PointField\nimport sensor_msgs_py.point_cloud2 as pc2\nimport numpy as np\n\nclass LidarProcessor(Node):\n    def __init__(self):\n        super().__init__(\'lidar_processor\')\n\n        # Subscribe to LiDAR point cloud\n        self.subscription = self.create_subscription(\n            PointCloud2,\n            \'/velodyne_points\',\n            self.lidar_callback,\n            10\n        )\n\n        # Publisher for processed obstacles\n        self.obstacle_pub = self.create_publisher(\n            PointCloud2,\n            \'/obstacles\',\n            10\n        )\n\n    def lidar_callback(self, msg):\n        # Convert ROS PointCloud2 to numpy array\n        points = self.pointcloud2_to_array(msg)\n\n        # Filter ground plane (assume flat floor at z=0)\n        non_ground = points[points[:, 2] > 0.1]  # Points above 10cm\n\n        # Detect obstacles within 2 meters\n        distances = np.linalg.norm(non_ground[:, :2], axis=1)\n        obstacles = non_ground[distances < 2.0]\n\n        # Cluster obstacles (simple spatial clustering)\n        clusters = self.dbscan_clustering(obstacles)\n\n        self.get_logger().info(f\'Detected {len(clusters)} obstacles\')\n\n        # Publish obstacle point cloud\n        obstacle_msg = self.array_to_pointcloud2(obstacles, msg.header)\n        self.obstacle_pub.publish(obstacle_msg)\n\n    def pointcloud2_to_array(self, cloud_msg):\n        """Convert PointCloud2 message to numpy array"""\n        points_list = []\n        for point in pc2.read_points(cloud_msg, skip_nans=True):\n            points_list.append([point[0], point[1], point[2]])\n        return np.array(points_list)\n\n    def array_to_pointcloud2(self, points, header):\n        """Convert numpy array to PointCloud2 message"""\n        return pc2.create_cloud_xyz32(header, points)\n\n    def dbscan_clustering(self, points, eps=0.3, min_samples=10):\n        """Simple DBSCAN clustering for obstacle grouping"""\n        from sklearn.cluster import DBSCAN\n\n        if len(points) == 0:\n            return []\n\n        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(points[:, :2])\n        labels = clustering.labels_\n\n        # Group points by cluster\n        clusters = []\n        for label in set(labels):\n            if label == -1:  # Noise\n                continue\n            cluster_points = points[labels == label]\n            clusters.append(cluster_points)\n\n        return clusters\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LidarProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"visualizing-lidar-data-in-rviz2",children:"Visualizing LiDAR Data in RViz2"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Launch RViz2\nros2 run rviz2 rviz2\n\n# Add PointCloud2 display\n# Topic: /velodyne_points\n# Fixed Frame: base_link\n# Color Transformer: AxisColor (colors by height)\n"})}),"\n",(0,t.jsx)(e.h2,{id:"depth-camera-simulation-rgb-d",children:"Depth Camera Simulation (RGB-D)"}),"\n",(0,t.jsx)(e.h3,{id:"rgb-d-camera-technology",children:"RGB-D Camera Technology"}),"\n",(0,t.jsx)(e.p,{children:"Depth cameras provide both color images and depth information. Technologies:"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Technology"}),(0,t.jsx)(e.th,{children:"Example"}),(0,t.jsx)(e.th,{children:"Range"}),(0,t.jsx)(e.th,{children:"Indoor/Outdoor"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Structured Light"})}),(0,t.jsx)(e.td,{children:"Kinect v1"}),(0,t.jsx)(e.td,{children:"0.5-4m"}),(0,t.jsx)(e.td,{children:"Indoor only"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Time-of-Flight"})}),(0,t.jsx)(e.td,{children:"Kinect Azure"}),(0,t.jsx)(e.td,{children:"0.25-5m"}),(0,t.jsx)(e.td,{children:"Indoor/Outdoor"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Stereo Vision"})}),(0,t.jsx)(e.td,{children:"Intel RealSense"}),(0,t.jsx)(e.td,{children:"0.1-10m"}),(0,t.jsx)(e.td,{children:"Both"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"LiDAR-based"})}),(0,t.jsx)(e.td,{children:"iPad Pro"}),(0,t.jsx)(e.td,{children:"0-5m"}),(0,t.jsx)(e.td,{children:"Both"})]})]})]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Humanoid Use Cases:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Object grasping (see depth for grip planning)"}),"\n",(0,t.jsx)(e.li,{children:"Facial recognition (RGB + depth for 3D face models)"}),"\n",(0,t.jsx)(e.li,{children:"Gesture detection (track hand depth for interaction)"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"simulating-rgb-d-in-gazebo",children:"Simulating RGB-D in Gazebo"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Intel RealSense D435 simulation --\x3e\n<gazebo reference="camera_link">\n  \x3c!-- RGB Camera --\x3e\n  <sensor name="realsense_color" type="camera">\n    <update_rate>30</update_rate>\n    <camera>\n      <horizontal_fov>1.211</horizontal_fov>  \x3c!-- 69.4\xb0 --\x3e\n      <image>\n        <width>1920</width>\n        <height>1080</height>\n        <format>RGB8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/realsense</namespace>\n        <remapping>image_raw:=color/image_raw</remapping>\n        <remapping>camera_info:=color/camera_info</remapping>\n      </ros>\n      <camera_name>color</camera_name>\n      <frame_name>camera_color_optical_frame</frame_name>\n    </plugin>\n  </sensor>\n\n  \x3c!-- Depth Camera --\x3e\n  <sensor name="realsense_depth" type="depth">\n    <update_rate>30</update_rate>\n    <camera>\n      <horizontal_fov>1.487</horizontal_fov>  \x3c!-- 85.2\xb0 --\x3e\n      <image>\n        <width>1280</width>\n        <height>720</height>\n        <format>R_FLOAT32</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10</far>\n      </clip>\n    </camera>\n    <plugin name="depth_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/realsense</namespace>\n        <remapping>image_raw:=depth/image_raw</remapping>\n        <remapping>camera_info:=depth/camera_info</remapping>\n      </ros>\n      <camera_name>depth</camera_name>\n      <frame_name>camera_depth_optical_frame</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,t.jsx)(e.h3,{id:"object-detection-with-rgb-d",children:"Object Detection with RGB-D"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass RGBDObjectDetector(Node):\n    def __init__(self):\n        super().__init__('rgbd_object_detector')\n\n        self.bridge = CvBridge()\n\n        # Subscribe to RGB and Depth images\n        self.rgb_sub = self.create_subscription(\n            Image,\n            '/realsense/color/image_raw',\n            self.rgb_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/realsense/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        self.latest_rgb = None\n        self.latest_depth = None\n\n        # Timer for synchronized processing\n        self.timer = self.create_timer(0.1, self.process_rgbd)\n\n    def rgb_callback(self, msg):\n        self.latest_rgb = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n    def depth_callback(self, msg):\n        self.latest_depth = self.bridge.imgmsg_to_cv2(msg, '32FC1')\n\n    def process_rgbd(self):\n        if self.latest_rgb is None or self.latest_depth is None:\n            return\n\n        # Detect objects by color (example: red objects)\n        hsv = cv2.cvtColor(self.latest_rgb, cv2.COLOR_BGR2HSV)\n\n        # Red color range\n        lower_red1 = np.array([0, 100, 100])\n        upper_red1 = np.array([10, 255, 255])\n        lower_red2 = np.array([160, 100, 100])\n        upper_red2 = np.array([180, 255, 255])\n\n        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n        mask = mask1 | mask2\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            if cv2.contourArea(contour) < 500:  # Filter small noise\n                continue\n\n            # Get bounding box\n            x, y, w, h = cv2.boundingRect(contour)\n\n            # Get depth at object center\n            center_x, center_y = x + w//2, y + h//2\n            depth = self.latest_depth[center_y, center_x]\n\n            # Draw on image\n            cv2.rectangle(self.latest_rgb, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            cv2.putText(\n                self.latest_rgb,\n                f'{depth:.2f}m',\n                (x, y-10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                (0, 255, 0),\n                2\n            )\n\n            self.get_logger().info(f'Red object detected at {depth:.2f}m')\n\n        # Display result\n        cv2.imshow('Object Detection', self.latest_rgb)\n        cv2.waitKey(1)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = RGBDObjectDetector()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"3d-point-cloud-from-rgb-d",children:"3D Point Cloud from RGB-D"}),"\n",(0,t.jsx)(e.p,{children:"Convert depth images to 3D point clouds for manipulation planning:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\n\ndef depth_to_pointcloud(depth_image, camera_info):\n    """\n    Convert depth image to 3D point cloud\n\n    Args:\n        depth_image: HxW numpy array of depth values (meters)\n        camera_info: CameraInfo message with intrinsics\n\n    Returns:\n        Nx3 numpy array of 3D points\n    """\n    h, w = depth_image.shape\n\n    # Camera intrinsic matrix\n    fx = camera_info.k[0]  # Focal length x\n    fy = camera_info.k[4]  # Focal length y\n    cx = camera_info.k[2]  # Principal point x\n    cy = camera_info.k[5]  # Principal point y\n\n    # Create meshgrid of pixel coordinates\n    u, v = np.meshgrid(np.arange(w), np.arange(h))\n\n    # Convert to 3D coordinates\n    z = depth_image\n    x = (u - cx) * z / fx\n    y = (v - cy) * z / fy\n\n    # Stack into Nx3 array\n    points = np.stack([x, y, z], axis=-1)\n    points = points.reshape(-1, 3)\n\n    # Remove invalid points (zero depth)\n    valid = points[:, 2] > 0\n    points = points[valid]\n\n    return points\n\n# Example usage\ncamera_info = get_camera_info()  # From /realsense/depth/camera_info\npoints_3d = depth_to_pointcloud(depth_image, camera_info)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"imu-simulation-inertial-measurement-unit",children:"IMU Simulation (Inertial Measurement Unit)"}),"\n",(0,t.jsx)(e.h3,{id:"imu-fundamentals",children:"IMU Fundamentals"}),"\n",(0,t.jsx)(e.p,{children:"IMUs measure:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Linear Acceleration"})," (accelerometer): 3-axis, includes gravity"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Angular Velocity"})," (gyroscope): 3-axis rotation rates"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Magnetic Field"})," (magnetometer): 3-axis compass heading"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Critical for Humanoid Robots:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Balance Control"}),": Detect falls before they happen"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Gait Stabilization"}),": Adjust walking pattern based on tilt"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Pose Estimation"}),": Combine with joint encoders for full-body state"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"simulating-imu-in-gazebo",children:"Simulating IMU in Gazebo"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'\x3c!-- IMU sensor in robot torso --\x3e\n<gazebo reference="torso_link">\n  <sensor name="imu_sensor" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n\n    <imu>\n      \x3c!-- Noise parameters based on real IMU specs (e.g., MPU-6050) --\x3e\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.009</stddev>  \x3c!-- 0.009 rad/s --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.009</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.009</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>  \x3c!-- 0.017 m/s\xb2 --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n\n    <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\n      <ros>\n        <namespace>/imu</namespace>\n        <remapping>~/out:=data</remapping>\n      </ros>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n      <frame_name>imu_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,t.jsx)(e.h3,{id:"fall-detection-with-imu",children:"Fall Detection with IMU"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nimport numpy as np\n\nclass FallDetector(Node):\n    def __init__(self):\n        super().__init__(\'fall_detector\')\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Thresholds for fall detection\n        self.tilt_threshold = 30.0  # degrees\n        self.acceleration_threshold = 20.0  # m/s\xb2 (high impact)\n\n    def imu_callback(self, msg):\n        # Extract orientation (quaternion to Euler)\n        orientation = msg.orientation\n        roll, pitch, yaw = self.quaternion_to_euler(\n            orientation.x, orientation.y, orientation.z, orientation.w\n        )\n\n        # Check if robot is tilted beyond safe angle\n        tilt_angle = np.degrees(max(abs(roll), abs(pitch)))\n\n        if tilt_angle > self.tilt_threshold:\n            self.get_logger().warn(f\'Tilt detected: {tilt_angle:.1f}\xb0\')\n            self.execute_fall_protection()\n\n        # Check for high impact (already fallen)\n        accel = msg.linear_acceleration\n        accel_magnitude = np.sqrt(accel.x**2 + accel.y**2 + accel.z**2)\n\n        if accel_magnitude > self.acceleration_threshold:\n            self.get_logger().error(\'High impact detected! Robot may have fallen.\')\n            self.execute_emergency_stop()\n\n    def quaternion_to_euler(self, x, y, z, w):\n        """Convert quaternion to Euler angles (roll, pitch, yaw)"""\n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = np.arctan2(sinr_cosp, cosr_cosp)\n\n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = np.copysign(np.pi / 2, sinp)\n        else:\n            pitch = np.arcsin(sinp)\n\n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = np.arctan2(siny_cosp, cosy_cosp)\n\n        return roll, pitch, yaw\n\n    def execute_fall_protection(self):\n        """Protective motions when fall is imminent"""\n        self.get_logger().info(\'Executing fall protection sequence...\')\n        # - Extend arms to catch fall\n        # - Bend knees to lower center of mass\n        # - Activate emergency motors to stiffen joints\n\n    def execute_emergency_stop(self):\n        """Cut power to motors after fall"""\n        self.get_logger().info(\'Emergency stop activated\')\n        # - Stop all motor commands\n        # - Log fall event\n        # - Request human assistance\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = FallDetector()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"sensor-fusion-complementary-filter",children:"Sensor Fusion: Complementary Filter"}),"\n",(0,t.jsx)(e.p,{children:"Combine accelerometer and gyroscope data for accurate orientation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class ComplementaryFilter(Node):\n    def __init__(self):\n        super().__init__('complementary_filter')\n\n        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)\n\n        # Filter parameters\n        self.alpha = 0.98  # Weight for gyroscope (fast but drifts)\n        self.beta = 0.02   # Weight for accelerometer (slow but accurate)\n\n        # State\n        self.roll = 0.0\n        self.pitch = 0.0\n        self.last_time = self.get_clock().now()\n\n    def imu_callback(self, msg):\n        current_time = self.get_clock().now()\n        dt = (current_time - self.last_time).nanoseconds / 1e9\n        self.last_time = current_time\n\n        # Gyroscope-based estimate (integrate angular velocity)\n        gyro_roll = self.roll + msg.angular_velocity.x * dt\n        gyro_pitch = self.pitch + msg.angular_velocity.y * dt\n\n        # Accelerometer-based estimate\n        accel_roll = np.arctan2(msg.linear_acceleration.y, msg.linear_acceleration.z)\n        accel_pitch = np.arctan2(-msg.linear_acceleration.x,\n                                  np.sqrt(msg.linear_acceleration.y**2 +\n                                          msg.linear_acceleration.z**2))\n\n        # Complementary filter (weighted average)\n        self.roll = self.alpha * gyro_roll + self.beta * accel_roll\n        self.pitch = self.alpha * gyro_pitch + self.beta * accel_pitch\n\n        self.get_logger().info(\n            f'Orientation - Roll: {np.degrees(self.roll):.1f}\xb0, '\n            f'Pitch: {np.degrees(self.pitch):.1f}\xb0'\n        )\n"})}),"\n",(0,t.jsx)(e.h2,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"}),"\n",(0,t.jsx)(e.h3,{id:"kalman-filter-for-sensor-integration",children:"Kalman Filter for Sensor Integration"}),"\n",(0,t.jsx)(e.p,{children:"Combine LiDAR, IMU, and wheel odometry for robust localization:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\n\nclass ExtendedKalmanFilter:\n    def __init__(self):\n        # State vector: [x, y, theta, v_x, v_y, omega]\n        self.state = np.zeros(6)\n\n        # Covariance matrix (uncertainty)\n        self.P = np.eye(6) * 0.1\n\n        # Process noise\n        self.Q = np.eye(6) * 0.01\n\n        # Measurement noise\n        self.R_imu = np.eye(3) * 0.1     # IMU: [theta, omega, accel]\n        self.R_odom = np.eye(3) * 0.05   # Odometry: [x, y, theta]\n        self.R_lidar = np.eye(2) * 0.02  # LiDAR landmarks: [x, y]\n\n    def predict(self, dt):\n        """Predict next state based on motion model"""\n        x, y, theta, vx, vy, omega = self.state\n\n        # State transition (constant velocity model)\n        self.state[0] += vx * dt  # x\n        self.state[1] += vy * dt  # y\n        self.state[2] += omega * dt  # theta\n\n        # Jacobian of motion model\n        F = np.eye(6)\n        F[0, 3] = dt\n        F[1, 4] = dt\n        F[2, 5] = dt\n\n        # Update covariance\n        self.P = F @ self.P @ F.T + self.Q\n\n    def update_imu(self, imu_data):\n        """Update with IMU measurement"""\n        # Measurement: [theta, omega, accel]\n        z = np.array([imu_data.orientation_z, imu_data.angular_velocity_z, imu_data.linear_acceleration_x])\n\n        # Measurement model\n        H = np.array([\n            [0, 0, 1, 0, 0, 0],  # theta\n            [0, 0, 0, 0, 0, 1],  # omega\n            [0, 0, 0, 1, 0, 0]   # accel \u2192 vx\n        ])\n\n        # Kalman gain\n        S = H @ self.P @ H.T + self.R_imu\n        K = self.P @ H.T @ np.linalg.inv(S)\n\n        # Update state\n        innovation = z - H @ self.state\n        self.state += K @ innovation\n\n        # Update covariance\n        self.P = (np.eye(6) - K @ H) @ self.P\n\n    def update_lidar(self, landmark_x, landmark_y):\n        """Update with LiDAR landmark detection"""\n        z = np.array([landmark_x, landmark_y])\n\n        H = np.array([\n            [1, 0, 0, 0, 0, 0],  # x\n            [0, 1, 0, 0, 0, 0]   # y\n        ])\n\n        S = H @ self.P @ H.T + self.R_lidar\n        K = self.P @ H.T @ np.linalg.inv(S)\n\n        innovation = z - H @ self.state\n        self.state += K @ innovation\n        self.P = (np.eye(6) - K @ H) @ self.P\n'})}),"\n",(0,t.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(e.h3,{id:"reducing-sensor-computational-load",children:"Reducing Sensor Computational Load"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class SensorOptimizer(Node):\n    def __init__(self):\n        super().__init__('sensor_optimizer')\n\n        # Adaptive update rates based on robot state\n        self.robot_state = 'stationary'  # or 'walking', 'running'\n\n    def adjust_sensor_rates(self):\n        if self.robot_state == 'stationary':\n            # Low update rates when not moving\n            self.set_lidar_rate(5)  # 5 Hz\n            self.set_camera_rate(10)  # 10 Hz\n            self.set_imu_rate(50)  # 50 Hz\n\n        elif self.robot_state == 'walking':\n            # Medium rates for walking\n            self.set_lidar_rate(10)\n            self.set_camera_rate(15)\n            self.set_imu_rate(100)\n\n        elif self.robot_state == 'running':\n            # High rates for dynamic motion\n            self.set_lidar_rate(20)\n            self.set_camera_rate(30)\n            self.set_imu_rate(200)\n"})}),"\n",(0,t.jsx)(e.h2,{id:"generating-synthetic-training-data",children:"Generating Synthetic Training Data"}),"\n",(0,t.jsx)(e.h3,{id:"automated-data-collection-in-simulation",children:"Automated Data Collection in Simulation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class DataCollector(Node):\n    def __init__(self):\n        super().__init__('data_collector')\n\n        self.rgb_sub = self.create_subscription(Image, '/camera/rgb', self.save_rgb, 10)\n        self.depth_sub = self.create_subscription(Image, '/camera/depth', self.save_depth, 10)\n        self.lidar_sub = self.create_subscription(PointCloud2, '/lidar', self.save_lidar, 10)\n\n        self.dataset_path = '/home/user/datasets/humanoid_sim/'\n        self.frame_count = 0\n\n    def save_rgb(self, msg):\n        rgb = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        filename = f'{self.dataset_path}/rgb/frame_{self.frame_count:06d}.png'\n        cv2.imwrite(filename, rgb)\n\n    def save_depth(self, msg):\n        depth = self.bridge.imgmsg_to_cv2(msg, '32FC1')\n        filename = f'{self.dataset_path}/depth/frame_{self.frame_count:06d}.npy'\n        np.save(filename, depth)\n\n    def save_lidar(self, msg):\n        points = self.pointcloud2_to_array(msg)\n        filename = f'{self.dataset_path}/lidar/frame_{self.frame_count:06d}.npy'\n        np.save(filename, points)\n\n        self.frame_count += 1\n\n        if self.frame_count % 100 == 0:\n            self.get_logger().info(f'Collected {self.frame_count} frames')\n"})}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"Sensor simulation is essential for developing robust humanoid robots:"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Key Takeaways:"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"LiDAR"}),": Use ",(0,t.jsx)(e.code,{children:"gpu_ray"})," in Gazebo for efficient 3D mapping"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"RGB-D Cameras"}),": Combine color and depth for object manipulation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"IMU"}),": Critical for balance and fall detection"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Fusion"}),": Combine multiple sensors with Kalman filtering"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Synthetic Data"}),": Generate training datasets in simulation"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Best Practices:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Always add realistic noise to simulated sensors"}),"\n",(0,t.jsx)(e.li,{children:"Tune noise parameters to match real hardware specs"}),"\n",(0,t.jsx)(e.li,{children:"Use sensor fusion instead of relying on single sensors"}),"\n",(0,t.jsx)(e.li,{children:"Optimize update rates based on robot state"}),"\n",(0,t.jsx)(e.li,{children:"Validate sim-to-real transfer with real hardware"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"By mastering sensor simulation, you can develop perception systems that work reliably when deployed on physical humanoid robots."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);