"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[3724],{8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var a=t(6540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}},8561:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module4-vla/cognitive-planning-ros2-actions","title":"Cognitive Planning (Translating Natural Language to ROS 2 Actions)","description":"Introduction: From Words to Executable Plans","source":"@site/docs/module4-vla/cognitive-planning-ros2-actions.md","sourceDirName":"module4-vla","slug":"/module4-vla/cognitive-planning-ros2-actions","permalink":"/physical-ai-textbook_01/docs/module4-vla/cognitive-planning-ros2-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/umernasir1/physical-ai-textbook/tree/main/docs/module4-vla/cognitive-planning-ros2-actions.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Cognitive Planning (Translating Natural Language to ROS 2 Actions)"},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action (OpenAI Whisper)","permalink":"/physical-ai-textbook_01/docs/module4-vla/voice-to-action-openai-whisper"},"next":{"title":"Capstone Project (The Autonomous Humanoid)","permalink":"/physical-ai-textbook_01/docs/module4-vla/capstone-autonomous-humanoid"}}');var i=t(4848),s=t(8453);const o={sidebar_position:3,title:"Cognitive Planning (Translating Natural Language to ROS 2 Actions)"},l="Cognitive Planning: Translating Natural Language to ROS 2 Actions",r={},c=[{value:"Introduction: From Words to Executable Plans",id:"introduction-from-words-to-executable-plans",level:2},{value:"The Cognitive Planning Challenge",id:"the-cognitive-planning-challenge",level:2},{value:"From High-Level Goals to Low-Level Actions",id:"from-high-level-goals-to-low-level-actions",level:3},{value:"Why Traditional Planning Falls Short",id:"why-traditional-planning-falls-short",level:3},{value:"LLM-Based Cognitive Planning",id:"llm-based-cognitive-planning",level:2},{value:"How LLMs Enable Flexible Planning",id:"how-llms-enable-flexible-planning",level:3},{value:"Architecture: LLM Planning Layer",id:"architecture-llm-planning-layer",level:3},{value:"Building the Cognitive Planner",id:"building-the-cognitive-planner",level:2},{value:"Step 1: Define Action Primitives",id:"step-1-define-action-primitives",level:3},{value:"Step 2: Create the LLM Planning Prompt",id:"step-2-create-the-llm-planning-prompt",level:3},{value:"Step 3: Scene Understanding Integration",id:"step-3-scene-understanding-integration",level:3},{value:"Step 4: ROS 2 Action Bridge",id:"step-4-ros-2-action-bridge",level:3},{value:"Step 5: Complete Integration Example",id:"step-5-complete-integration-example",level:3},{value:"Advanced Planning Techniques",id:"advanced-planning-techniques",level:2},{value:"1. Hierarchical Task Planning",id:"1-hierarchical-task-planning",level:3},{value:"2. Dynamic Replanning",id:"2-dynamic-replanning",level:3},{value:"3. Multi-Modal Reasoning",id:"3-multi-modal-reasoning",level:3},{value:"4. Learning from Failures",id:"4-learning-from-failures",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Plan Validation Before Execution",id:"plan-validation-before-execution",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Latency Optimization",id:"latency-optimization",level:3},{value:"Real-World Example: Kitchen Assistant Robot",id:"real-world-example-kitchen-assistant-robot",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Learning Objectives Achieved",id:"learning-objectives-achieved",level:2}];function d(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"cognitive-planning-translating-natural-language-to-ros-2-actions",children:"Cognitive Planning: Translating Natural Language to ROS 2 Actions"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction-from-words-to-executable-plans",children:"Introduction: From Words to Executable Plans"}),"\n",(0,i.jsx)(n.p,{children:'The true power of Language-Vision-Action (VLA) systems emerges when robots can understand high-level human instructions and autonomously decompose them into executable action sequences. Instead of programming every step, we simply tell the robot: "Clean the room," and it figures out the rest.'}),"\n",(0,i.jsxs)(n.p,{children:["This chapter explores ",(0,i.jsx)(n.strong,{children:"cognitive planning"}),"\u2014using Large Language Models (LLMs) to translate natural language commands into structured ROS 2 action sequences that robots can execute."]}),"\n",(0,i.jsx)(n.h2,{id:"the-cognitive-planning-challenge",children:"The Cognitive Planning Challenge"}),"\n",(0,i.jsx)(n.h3,{id:"from-high-level-goals-to-low-level-actions",children:"From High-Level Goals to Low-Level Actions"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Human Instruction:"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:'"Make me a cup of coffee"'}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What the Robot Must Figure Out:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Navigate to the kitchen"}),"\n",(0,i.jsx)(n.li,{children:"Locate the coffee machine"}),"\n",(0,i.jsx)(n.li,{children:"Check if there's a cup nearby"}),"\n",(0,i.jsx)(n.li,{children:"If no cup: search for cup, grasp cup, place under spout"}),"\n",(0,i.jsx)(n.li,{children:"Verify coffee machine has water and beans"}),"\n",(0,i.jsx)(n.li,{children:"Press the brew button"}),"\n",(0,i.jsx)(n.li,{children:"Wait for brewing to complete"}),"\n",(0,i.jsx)(n.li,{children:"Grasp the filled cup"}),"\n",(0,i.jsx)(n.li,{children:"Navigate to the user"}),"\n",(0,i.jsx)(n.li,{children:"Hand over the cup safely"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"The Challenge:"})," Bridging the semantic gap between human intent and robot capabilities."]}),"\n",(0,i.jsx)(n.h3,{id:"why-traditional-planning-falls-short",children:"Why Traditional Planning Falls Short"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Rule-Based Systems:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Brittle and inflexible\nif command == "make coffee":\n    navigate("kitchen")\n    find_object("coffee_machine")\n    press_button("brew")\n    # Breaks if coffee machine is different or cup is missing\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Cannot handle variations or exceptions"}),"\n",(0,i.jsx)(n.li,{children:"Requires explicit programming for every scenario"}),"\n",(0,i.jsx)(n.li,{children:"No common-sense reasoning"}),"\n",(0,i.jsx)(n.li,{children:"Poor generalization to new situations"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"llm-based-cognitive-planning",children:"LLM-Based Cognitive Planning"}),"\n",(0,i.jsx)(n.h3,{id:"how-llms-enable-flexible-planning",children:"How LLMs Enable Flexible Planning"}),"\n",(0,i.jsx)(n.p,{children:"LLMs bring several critical capabilities:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"World Knowledge"}),": Understanding of objects, their properties, and typical uses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Common-Sense Reasoning"}),': Inferring implicit requirements (e.g., "coffee needs a cup")']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Decomposition"}),": Breaking complex goals into logical steps"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Contextual Adaptation"}),": Adjusting plans based on environment state"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Recovery"}),": Generating alternative plans when failures occur"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"architecture-llm-planning-layer",children:"Architecture: LLM Planning Layer"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"[Natural Language Command]\n         \u2193\n[LLM Planner] \u2190 [Scene Understanding] \u2190 [Robot Sensors]\n         \u2193\n[Structured Action Plan (JSON)]\n         \u2193\n[ROS 2 Action Bridge]\n         \u2193\n[ROS 2 Action Servers] \u2192 [Robot Execution]\n"})}),"\n",(0,i.jsx)(n.h2,{id:"building-the-cognitive-planner",children:"Building the Cognitive Planner"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-define-action-primitives",children:"Step 1: Define Action Primitives"}),"\n",(0,i.jsx)(n.p,{children:"First, define the low-level actions your robot can perform:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# action_primitives.py\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, List\n\n@dataclass\nclass ActionPrimitive:\n    """Base class for robot actions"""\n    action_type: str\n    parameters: Dict[str, Any]\n    preconditions: List[str]\n    effects: List[str]\n\n# Define available actions\nAVAILABLE_ACTIONS = {\n    "navigate": ActionPrimitive(\n        action_type="navigate",\n        parameters={"target": "pose or location name"},\n        preconditions=["robot_is_mobile"],\n        effects=["robot_at_target"]\n    ),\n    "grasp": ActionPrimitive(\n        action_type="grasp",\n        parameters={"object": "object_id", "approach": "top|side"},\n        preconditions=["object_in_reach", "gripper_empty"],\n        effects=["object_grasped"]\n    ),\n    "place": ActionPrimitive(\n        action_type="place",\n        parameters={"location": "target_surface"},\n        preconditions=["object_grasped"],\n        effects=["object_placed", "gripper_empty"]\n    ),\n    "detect": ActionPrimitive(\n        action_type="detect",\n        parameters={"object_class": "object category"},\n        preconditions=["camera_active"],\n        effects=["object_located"]\n    ),\n    "manipulate": ActionPrimitive(\n        action_type="manipulate",\n        parameters={"device": "button|switch|handle", "action": "press|flip|turn"},\n        preconditions=["device_in_reach"],\n        effects=["device_state_changed"]\n    )\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-create-the-llm-planning-prompt",children:"Step 2: Create the LLM Planning Prompt"}),"\n",(0,i.jsx)(n.p,{children:"Design a prompt that guides the LLM to generate valid plans:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# cognitive_planner.py\nimport json\nimport openai\nfrom typing import List, Dict\n\nclass CognitivePlanner:\n    def __init__(self, model="gpt-4"):\n        self.model = model\n        self.action_library = AVAILABLE_ACTIONS\n\n    def create_system_prompt(self):\n        actions_json = json.dumps(\n            {k: v.__dict__ for k, v in self.action_library.items()},\n            indent=2\n        )\n\n        return f"""You are a robotic task planner. Given a high-level user command and the current environment state, generate a detailed action sequence.\n\nAvailable Actions:\n{actions_json}\n\nOutput Format (JSON):\n{{\n  "plan": [\n    {{\n      "step": 1,\n      "action": "action_type",\n      "parameters": {{}},\n      "reasoning": "why this step is needed"\n    }}\n  ],\n  "preconditions_check": ["condition1", "condition2"],\n  "estimated_duration": "time in seconds",\n  "failure_recovery": "what to do if plan fails"\n}}\n\nRules:\n1. Break complex tasks into atomic actions\n2. Check preconditions before actions\n3. Consider object relationships and physics\n4. Plan for error cases\n5. Minimize unnecessary movements\n6. Ensure safety (e.g., check for humans before moving)\n"""\n\n    def generate_plan(self, user_command: str, scene_state: Dict) -> Dict:\n        """Generate action plan from natural language command"""\n\n        user_prompt = f"""\nUser Command: "{user_command}"\n\nCurrent Environment State:\n{json.dumps(scene_state, indent=2)}\n\nGenerate a detailed action plan to accomplish this task.\n"""\n\n        response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": self.create_system_prompt()},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=0.2  # Lower temperature for more consistent plans\n        )\n\n        plan = json.loads(response.choices[0].message.content)\n        return self.validate_plan(plan)\n\n    def validate_plan(self, plan: Dict) -> Dict:\n        """Validate that plan uses only available actions"""\n        for step in plan[\'plan\']:\n            if step[\'action\'] not in self.action_library:\n                raise ValueError(f"Invalid action: {step[\'action\']}")\n\n            # Check parameter requirements\n            required_params = self.action_library[step[\'action\']].parameters\n            for param in required_params:\n                if param not in step[\'parameters\']:\n                    raise ValueError(f"Missing parameter {param} for {step[\'action\']}")\n\n        return plan\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-scene-understanding-integration",children:"Step 3: Scene Understanding Integration"}),"\n",(0,i.jsx)(n.p,{children:"Provide the LLM with current environment information:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# scene_understanding.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom vision_msgs.msg import Detection3DArray\nimport cv2\nimport numpy as np\n\nclass SceneUnderstanding(Node):\n    def __init__(self):\n        super().__init__(\'scene_understanding\')\n\n        # Subscribers for perception data\n        self.camera_sub = self.create_subscription(\n            Image, \'/camera/color/image_raw\', self.image_callback, 10\n        )\n        self.detection_sub = self.create_subscription(\n            Detection3DArray, \'/detected_objects\', self.detection_callback, 10\n        )\n\n        self.detected_objects = []\n        self.robot_pose = None\n\n    def get_scene_state(self) -> Dict:\n        """Generate structured scene description for LLM"""\n        return {\n            "robot_location": {\n                "position": self.robot_pose[\'position\'] if self.robot_pose else "unknown",\n                "orientation": self.robot_pose[\'orientation\'] if self.robot_pose else "unknown"\n            },\n            "detected_objects": [\n                {\n                    "id": obj.id,\n                    "class": obj.object_class,\n                    "position": {\n                        "x": obj.pose.position.x,\n                        "y": obj.pose.position.y,\n                        "z": obj.pose.position.z\n                    },\n                    "distance": self.calculate_distance(obj.pose),\n                    "graspable": self.is_graspable(obj)\n                }\n                for obj in self.detected_objects\n            ],\n            "nearby_locations": ["kitchen", "living_room", "table", "counter"],\n            "gripper_state": "empty",  # or "holding_object"\n            "battery_level": 85,\n            "obstacles": self.detect_obstacles()\n        }\n\n    def detection_callback(self, msg):\n        self.detected_objects = msg.detections\n\n    def is_graspable(self, obj) -> bool:\n        """Determine if object can be grasped"""\n        # Check size, position, etc.\n        size = obj.bbox.size\n        return (0.02 < size.x < 0.3 and\n                0.02 < size.y < 0.3 and\n                0.02 < size.z < 0.3)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-ros-2-action-bridge",children:"Step 4: ROS 2 Action Bridge"}),"\n",(0,i.jsx)(n.p,{children:"Translate LLM plans into ROS 2 actions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# action_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nfrom control_msgs.action import GripperCommand\nfrom manipulation_msgs.action import PickupObject, PlaceObject\n\nclass ActionBridge(Node):\n    def __init__(self):\n        super().__init__('action_bridge')\n\n        # Create action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.pickup_client = ActionClient(self, PickupObject, 'pickup_object')\n        self.place_client = ActionClient(self, PlaceObject, 'place_object')\n        self.gripper_client = ActionClient(self, GripperCommand, 'gripper_controller')\n\n    async def execute_plan(self, plan: Dict):\n        \"\"\"Execute LLM-generated plan step by step\"\"\"\n        for step in plan['plan']:\n            self.get_logger().info(f\"Executing step {step['step']}: {step['action']}\")\n            self.get_logger().info(f\"Reasoning: {step['reasoning']}\")\n\n            success = await self.execute_action(step)\n\n            if not success:\n                self.get_logger().error(f\"Step {step['step']} failed\")\n                # Attempt recovery\n                recovery_success = await self.attempt_recovery(step, plan)\n                if not recovery_success:\n                    self.get_logger().error(\"Recovery failed, aborting plan\")\n                    return False\n\n        self.get_logger().info(\"Plan executed successfully\")\n        return True\n\n    async def execute_action(self, step: Dict) -> bool:\n        \"\"\"Execute single action\"\"\"\n        action_type = step['action']\n        params = step['parameters']\n\n        if action_type == 'navigate':\n            return await self.navigate(params['target'])\n        elif action_type == 'grasp':\n            return await self.grasp(params['object'])\n        elif action_type == 'place':\n            return await self.place(params['location'])\n        elif action_type == 'detect':\n            return await self.detect(params['object_class'])\n        elif action_type == 'manipulate':\n            return await self.manipulate(params['device'], params['action'])\n        else:\n            self.get_logger().warn(f\"Unknown action: {action_type}\")\n            return False\n\n    async def navigate(self, target) -> bool:\n        \"\"\"Execute navigation action\"\"\"\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = self.lookup_pose(target)\n\n        self.get_logger().info(f'Navigating to {target}')\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\n        goal_handle = await send_goal_future\n\n        if not goal_handle.accepted:\n            self.get_logger().error('Navigation goal rejected')\n            return False\n\n        result = await goal_handle.get_result_async()\n        return result.result.error_code == 0\n\n    async def grasp(self, object_id) -> bool:\n        \"\"\"Execute grasp action\"\"\"\n        goal_msg = PickupObject.Goal()\n        goal_msg.object_id = object_id\n\n        self.get_logger().info(f'Grasping object {object_id}')\n        send_goal_future = self.pickup_client.send_goal_async(goal_msg)\n        goal_handle = await send_goal_future\n\n        if not goal_handle.accepted:\n            return False\n\n        result = await goal_handle.get_result_async()\n        return result.result.success\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-5-complete-integration-example",children:"Step 5: Complete Integration Example"}),"\n",(0,i.jsx)(n.p,{children:"Putting it all together:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# cognitive_robot_controller.py\nimport rclpy\nfrom rclpy.node import Node\nimport asyncio\n\nclass CognitiveRobotController(Node):\n    def __init__(self):\n        super().__init__('cognitive_robot_controller')\n\n        # Initialize components\n        self.planner = CognitivePlanner()\n        self.scene_understanding = SceneUnderstanding()\n        self.action_bridge = ActionBridge()\n\n        self.get_logger().info('Cognitive robot controller initialized')\n\n    async def execute_natural_language_command(self, command: str):\n        \"\"\"Main pipeline: Command \u2192 Plan \u2192 Execute\"\"\"\n\n        self.get_logger().info(f'Received command: {command}')\n\n        # 1. Understand current scene\n        scene_state = self.scene_understanding.get_scene_state()\n        self.get_logger().info(f'Scene state: {scene_state}')\n\n        # 2. Generate plan with LLM\n        self.get_logger().info('Generating plan with LLM...')\n        plan = self.planner.generate_plan(command, scene_state)\n        self.get_logger().info(f'Generated plan: {json.dumps(plan, indent=2)}')\n\n        # 3. Execute plan\n        self.get_logger().info('Executing plan...')\n        success = await self.action_bridge.execute_plan(plan)\n\n        if success:\n            self.get_logger().info('Task completed successfully')\n        else:\n            self.get_logger().error('Task execution failed')\n\n        return success\n\nasync def main():\n    rclpy.init()\n    controller = CognitiveRobotController()\n\n    # Example commands\n    commands = [\n        \"Pick up the red block and place it on the table\",\n        \"Go to the kitchen and bring me a cup\",\n        \"Find all the books and stack them on the shelf\"\n    ]\n\n    for cmd in commands:\n        await controller.execute_natural_language_command(cmd)\n        await asyncio.sleep(2)\n\n    controller.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-planning-techniques",children:"Advanced Planning Techniques"}),"\n",(0,i.jsx)(n.h3,{id:"1-hierarchical-task-planning",children:"1. Hierarchical Task Planning"}),"\n",(0,i.jsx)(n.p,{children:"Break complex tasks into hierarchies:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def generate_hierarchical_plan(self, command: str, scene_state: Dict) -> Dict:\n    """Generate multi-level plan"""\n\n    # Level 1: High-level goals\n    high_level_prompt = f"""\n    Break down this task into 3-5 major subtasks:\n    Task: {command}\n\n    Output as JSON list of subtasks.\n    """\n\n    subtasks = self.llm_query(high_level_prompt)\n\n    # Level 2: Detailed actions for each subtask\n    detailed_plan = []\n    for subtask in subtasks:\n        detailed_actions = self.generate_plan(subtask, scene_state)\n        detailed_plan.append({\n            "subtask": subtask,\n            "actions": detailed_actions\n        })\n\n    return detailed_plan\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-dynamic-replanning",children:"2. Dynamic Replanning"}),"\n",(0,i.jsx)(n.p,{children:"Adapt plans when environment changes:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class AdaptivePlanner(CognitivePlanner):\n    def __init__(self):\n        super().__init__()\n        self.current_plan = None\n        self.executed_steps = []\n\n    async def execute_with_replanning(self, command: str):\n        """Execute plan with dynamic replanning"""\n\n        while not self.task_completed():\n            # Get current state\n            scene_state = self.scene_understanding.get_scene_state()\n\n            # Generate/update plan\n            if self.needs_replanning(scene_state):\n                self.get_logger().info(\'Replanning due to environment change\')\n                self.current_plan = self.planner.generate_plan(\n                    command,\n                    scene_state,\n                    completed_steps=self.executed_steps\n                )\n\n            # Execute next step\n            next_step = self.current_plan[\'plan\'][len(self.executed_steps)]\n            success = await self.action_bridge.execute_action(next_step)\n\n            if success:\n                self.executed_steps.append(next_step)\n            else:\n                # Trigger replanning\n                self.current_plan = None\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-multi-modal-reasoning",children:"3. Multi-Modal Reasoning"}),"\n",(0,i.jsx)(n.p,{children:"Combine vision and language for better understanding:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def generate_multimodal_plan(self, command: str, image: np.ndarray, scene_state: Dict):\n    """Use both vision and language"""\n\n    # Encode image\n    vision_model = VisionTransformer()\n    visual_features = vision_model.encode(image)\n\n    # Create multimodal prompt\n    prompt = f"""\n    Task: {command}\n    Scene state: {scene_state}\n    Visual features: {visual_features}\n\n    Based on both the textual description and visual information,\n    generate an action plan.\n    """\n\n    return self.llm_query(prompt)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"4-learning-from-failures",children:"4. Learning from Failures"}),"\n",(0,i.jsx)(n.p,{children:"Improve plans based on execution outcomes:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class LearningPlanner(CognitivePlanner):\n    def __init__(self):\n        super().__init__()\n        self.experience_memory = []\n\n    def record_experience(self, command: str, plan: Dict, outcome: bool):\n        """Store execution experience"""\n        self.experience_memory.append({\n            "command": command,\n            "plan": plan,\n            "success": outcome,\n            "timestamp": time.time()\n        })\n\n    def generate_plan_with_experience(self, command: str, scene_state: Dict):\n        """Use past experiences to improve planning"""\n\n        # Find similar past tasks\n        similar_experiences = self.find_similar_tasks(command)\n\n        prompt = f"""\n        Task: {command}\n        Scene: {scene_state}\n\n        Past similar tasks and outcomes:\n        {json.dumps(similar_experiences, indent=2)}\n\n        Generate a plan that learns from past successes and failures.\n        """\n\n        return self.llm_query(prompt)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,i.jsx)(n.h3,{id:"plan-validation-before-execution",children:"Plan Validation Before Execution"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SafetyValidator:\n    def __init__(self):\n        self.safety_rules = self.load_safety_rules()\n\n    def validate_plan(self, plan: Dict, scene_state: Dict) -> tuple[bool, str]:\n        \"\"\"Validate plan for safety\"\"\"\n\n        for step in plan['plan']:\n            # Check collision risk\n            if self.has_collision_risk(step, scene_state):\n                return False, f\"Collision risk in step {step['step']}\"\n\n            # Check force limits\n            if step['action'] == 'manipulate':\n                if not self.validate_force_limits(step):\n                    return False, \"Excessive force detected\"\n\n            # Check human proximity\n            if self.human_too_close(step, scene_state):\n                return False, \"Human in workspace\"\n\n            # Check workspace boundaries\n            if not self.in_workspace(step):\n                return False, \"Action outside safe workspace\"\n\n        return True, \"Plan validated\"\n\n    def human_too_close(self, step: Dict, scene_state: Dict) -> bool:\n        \"\"\"Check if humans are too close to planned action\"\"\"\n        human_objects = [obj for obj in scene_state['detected_objects']\n                        if obj['class'] == 'person']\n\n        action_location = step['parameters'].get('target') or step['parameters'].get('location')\n\n        for human in human_objects:\n            distance = self.calculate_distance(action_location, human['position'])\n            if distance < 0.5:  # 50cm safety margin\n                return True\n\n        return False\n"})}),"\n",(0,i.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"latency-optimization",children:"Latency Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class FastCognitivePlanner:\n    def __init__(self):\n        self.planner = CognitivePlanner()\n        self.plan_cache = {}\n\n    def generate_plan_fast(self, command: str, scene_state: Dict):\n        """Optimize planning latency"""\n\n        # 1. Check cache for similar commands\n        cache_key = self.compute_cache_key(command, scene_state)\n        if cache_key in self.plan_cache:\n            cached_plan = self.plan_cache[cache_key]\n            # Adapt cached plan to current state\n            return self.adapt_plan(cached_plan, scene_state)\n\n        # 2. Use smaller, faster LLM for simple commands\n        if self.is_simple_command(command):\n            plan = self.planner.generate_plan(command, scene_state, model="gpt-3.5-turbo")\n        else:\n            plan = self.planner.generate_plan(command, scene_state, model="gpt-4")\n\n        # 3. Cache the plan\n        self.plan_cache[cache_key] = plan\n\n        return plan\n'})}),"\n",(0,i.jsx)(n.h2,{id:"real-world-example-kitchen-assistant-robot",children:"Real-World Example: Kitchen Assistant Robot"}),"\n",(0,i.jsx)(n.p,{children:"Complete implementation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# kitchen_assistant.py\nclass KitchenAssistant(CognitiveRobotController):\n    def __init__(self):\n        super().__init__()\n        self.kitchen_objects = [\'cup\', \'plate\', \'spoon\', \'coffee_machine\', \'fridge\']\n\n    async def process_cooking_command(self, command: str):\n        """Handle cooking-related commands"""\n\n        # Enhanced scene understanding for kitchen\n        scene_state = self.scene_understanding.get_scene_state()\n        scene_state[\'kitchen_appliances\'] = self.detect_appliances()\n        scene_state[\'ingredients\'] = self.detect_ingredients()\n\n        # Generate specialized kitchen plan\n        system_prompt = self.get_kitchen_prompt()\n        plan = self.planner.generate_plan(command, scene_state, system_prompt)\n\n        # Execute with kitchen-specific safety checks\n        return await self.execute_kitchen_plan(plan)\n\n    def get_kitchen_prompt(self):\n        return """You are a kitchen assistant robot. Consider:\n        - Food safety (proper temperatures, cross-contamination)\n        - Appliance operation (preheating, timing)\n        - Ingredient preparation order\n        - Cleanup procedures\n        """\n\n# Example usage\nasync def demo():\n    robot = KitchenAssistant()\n\n    commands = [\n        "Make me a sandwich",\n        "Heat up the leftovers in the microwave",\n        "Clean the counter and put away the dishes"\n    ]\n\n    for cmd in commands:\n        await robot.process_cooking_command(cmd)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cognitive planning bridges high-level intent and low-level actions"})," using LLM reasoning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Structured prompts guide LLMs"})," to generate valid, executable plans"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scene understanding provides critical context"})," for planning decisions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 actions serve as the execution layer"})," for LLM-generated plans"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety validation is mandatory"})," before executing any plan"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dynamic replanning enables adaptation"})," to changing environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Experience memory improves planning"})," over time"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives-achieved",children:"Learning Objectives Achieved"}),"\n",(0,i.jsx)(n.p,{children:"By completing this chapter, you should be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design cognitive planning systems using LLMs"}),"\n",(0,i.jsx)(n.li,{children:"Create structured prompts for task decomposition"}),"\n",(0,i.jsx)(n.li,{children:"Integrate scene understanding with planning"}),"\n",(0,i.jsx)(n.li,{children:"Bridge LLM outputs to ROS 2 actions"}),"\n",(0,i.jsx)(n.li,{children:"Implement safety validation for AI-generated plans"}),"\n",(0,i.jsx)(n.li,{children:"Build adaptive replanning systems"}),"\n",(0,i.jsx)(n.li,{children:"Optimize planning performance for real-time robotics"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": In the capstone project, we'll integrate everything\u2014vision, language, and action\u2014to build a fully autonomous humanoid robot."]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);