"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[101],{1926:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4-vla/voice-to-action-openai-whisper","title":"Voice-to-Action (OpenAI Whisper)","description":"Introduction: Natural Voice Control for Robots","source":"@site/docs/module4-vla/voice-to-action-openai-whisper.md","sourceDirName":"module4-vla","slug":"/module4-vla/voice-to-action-openai-whisper","permalink":"/-physical-ai-textbook_01/docs/module4-vla/voice-to-action-openai-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/umernasir1/physical-ai-textbook/tree/main/docs/module4-vla/voice-to-action-openai-whisper.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Voice-to-Action (OpenAI Whisper)"},"sidebar":"tutorialSidebar","previous":{"title":"The Convergence of LLMs and Robotics","permalink":"/-physical-ai-textbook_01/docs/module4-vla/llms-and-robotics-convergence"},"next":{"title":"Cognitive Planning (Translating Natural Language to ROS 2 Actions)","permalink":"/-physical-ai-textbook_01/docs/module4-vla/cognitive-planning-ros2-actions"}}');var s=i(4848),r=i(8453);const o={sidebar_position:2,title:"Voice-to-Action (OpenAI Whisper)"},a="Voice-to-Action with OpenAI Whisper",l={},c=[{value:"Introduction: Natural Voice Control for Robots",id:"introduction-natural-voice-control-for-robots",level:2},{value:"Why Voice Control Matters for Robotics",id:"why-voice-control-matters-for-robotics",level:2},{value:"Advantages Over Traditional Interfaces",id:"advantages-over-traditional-interfaces",level:3},{value:"Real-World Use Cases",id:"real-world-use-cases",level:3},{value:"OpenAI Whisper: State-of-the-Art Speech Recognition",id:"openai-whisper-state-of-the-art-speech-recognition",level:2},{value:"What Makes Whisper Special",id:"what-makes-whisper-special",level:3},{value:"Whisper Model Architecture",id:"whisper-model-architecture",level:3},{value:"Model Sizes and Trade-offs",id:"model-sizes-and-trade-offs",level:3},{value:"Building a Voice-to-Action Pipeline",id:"building-a-voice-to-action-pipeline",level:2},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Step 1: Audio Capture with ROS 2",id:"step-1-audio-capture-with-ros-2",level:3},{value:"Step 2: Whisper Integration",id:"step-2-whisper-integration",level:3},{value:"Step 3: Intent Recognition and Command Parsing",id:"step-3-intent-recognition-and-command-parsing",level:3},{value:"Step 4: Connecting to Robot Actions",id:"step-4-connecting-to-robot-actions",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"1. Wake Word Detection",id:"1-wake-word-detection",level:3},{value:"2. Multilingual Support",id:"2-multilingual-support",level:3},{value:"3. Confidence-Based Filtering",id:"3-confidence-based-filtering",level:3},{value:"4. Contextual Understanding with LLMs",id:"4-contextual-understanding-with-llms",level:3},{value:"Deployment on Edge Devices",id:"deployment-on-edge-devices",level:2},{value:"Running Whisper on NVIDIA Jetson",id:"running-whisper-on-nvidia-jetson",level:3},{value:"Handling Edge Cases",id:"handling-edge-cases",level:2},{value:"1. Background Noise",id:"1-background-noise",level:3},{value:"2. Overlapping Speech",id:"2-overlapping-speech",level:3},{value:"3. Ambiguous Commands",id:"3-ambiguous-commands",level:3},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"1. Emergency Stop",id:"1-emergency-stop",level:3},{value:"2. Command Confirmation",id:"2-command-confirmation",level:3},{value:"Complete End-to-End Example",id:"complete-end-to-end-example",level:2},{value:"Performance Optimization Tips",id:"performance-optimization-tips",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Learning Objectives Achieved",id:"learning-objectives-achieved",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction-natural-voice-control-for-robots",children:"Introduction: Natural Voice Control for Robots"}),"\n",(0,s.jsx)(n.p,{children:'Voice control represents the most natural interface for human-robot interaction. Instead of typing commands or programming sequences, users simply speak their intentions: "Pick up the red block," "Follow me," or "Clean the kitchen." OpenAI Whisper, a state-of-the-art speech recognition model, enables robust voice-to-action systems that work across accents, languages, and noisy environments.'}),"\n",(0,s.jsx)(n.p,{children:"This chapter explores how to integrate Whisper with ROS 2 to create voice-controlled humanoid robots."}),"\n",(0,s.jsx)(n.h2,{id:"why-voice-control-matters-for-robotics",children:"Why Voice Control Matters for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"advantages-over-traditional-interfaces",children:"Advantages Over Traditional Interfaces"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hands-Free Operation"}),": Users can control robots while performing other tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Interaction"}),": Speaking is more intuitive than typing or programming"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accessibility"}),": Enables robot control for users with limited mobility"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speed"}),": Voice commands are faster than navigating complex interfaces"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Contextual Communication"}),": Tone and emphasis convey additional meaning"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"real-world-use-cases",children:"Real-World Use Cases"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manufacturing"}),': "Robot, hand me the 10mm wrench"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Healthcare"}),': "Bring wheelchair to patient in bed 3"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Elderly Care"}),': "Remind me to take medication at 3 PM"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emergency Response"}),': "Search for survivors in sector B"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Home Assistance"}),': "Start cleaning while I\'m at work"']}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"openai-whisper-state-of-the-art-speech-recognition",children:"OpenAI Whisper: State-of-the-Art Speech Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"what-makes-whisper-special",children:"What Makes Whisper Special"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Traditional ASR (Automatic Speech Recognition) vs. Whisper:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"Traditional ASR"}),(0,s.jsx)(n.th,{children:"Whisper"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Training Data"}),(0,s.jsx)(n.td,{children:"~1,000 hours"}),(0,s.jsx)(n.td,{children:"680,000 hours"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Languages"}),(0,s.jsx)(n.td,{children:"Limited"}),(0,s.jsx)(n.td,{children:"99 languages"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Robustness"}),(0,s.jsx)(n.td,{children:"Poor in noise"}),(0,s.jsx)(n.td,{children:"Excellent"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Accuracy"}),(0,s.jsx)(n.td,{children:"~80-90%"}),(0,s.jsx)(n.td,{children:"~95-98%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Accents"}),(0,s.jsx)(n.td,{children:"Struggles"}),(0,s.jsx)(n.td,{children:"Handles well"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"whisper-model-architecture",children:"Whisper Model Architecture"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Encoder-Decoder Transformer:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Encoder"}),": Processes audio waveform into features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decoder"}),": Generates text tokens from audio features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multitask Training"}),": Transcription, translation, language detection, timestamp prediction"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"model-sizes-and-trade-offs",children:"Model Sizes and Trade-offs"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"Relative Speed"}),(0,s.jsx)(n.th,{children:"Relative Accuracy"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"tiny"}),(0,s.jsx)(n.td,{children:"39M"}),(0,s.jsx)(n.td,{children:"32x"}),(0,s.jsx)(n.td,{children:"Good"}),(0,s.jsx)(n.td,{children:"Edge devices (Jetson)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"base"}),(0,s.jsx)(n.td,{children:"74M"}),(0,s.jsx)(n.td,{children:"16x"}),(0,s.jsx)(n.td,{children:"Better"}),(0,s.jsx)(n.td,{children:"Real-time applications"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"small"}),(0,s.jsx)(n.td,{children:"244M"}),(0,s.jsx)(n.td,{children:"6x"}),(0,s.jsx)(n.td,{children:"Great"}),(0,s.jsx)(n.td,{children:"Balanced performance"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"medium"}),(0,s.jsx)(n.td,{children:"769M"}),(0,s.jsx)(n.td,{children:"2x"}),(0,s.jsx)(n.td,{children:"Excellent"}),(0,s.jsx)(n.td,{children:"High accuracy needed"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"large"}),(0,s.jsx)(n.td,{children:"1550M"}),(0,s.jsx)(n.td,{children:"1x"}),(0,s.jsx)(n.td,{children:"Best"}),(0,s.jsx)(n.td,{children:"Offline processing"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["For robotics, we typically use ",(0,s.jsx)(n.code,{children:"small"})," or ",(0,s.jsx)(n.code,{children:"base"})," models for real-time performance."]})}),"\n",(0,s.jsx)(n.h2,{id:"building-a-voice-to-action-pipeline",children:"Building a Voice-to-Action Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'[Microphone] \u2192 [Audio Capture] \u2192 [Whisper STT] \u2192 [Intent Parser] \u2192 [ROS 2 Actions]\n     \u2193              \u2193                  \u2193                \u2193                  \u2193\n  Hardware      Audio Buffer      "Pick up cup"    ActionGoal        Robot Moves\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-1-audio-capture-with-ros-2",children:"Step 1: Audio Capture with ROS 2"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ROS 2 Audio Capture Node:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom audio_common_msgs.msg import AudioData\nimport pyaudio\nimport numpy as np\n\nclass AudioCaptureNode(Node):\n    def __init__(self):\n        super().__init__('audio_capture')\n        self.publisher = self.create_publisher(AudioData, 'audio/input', 10)\n\n        # PyAudio configuration\n        self.audio_format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000  # Whisper expects 16kHz\n        self.chunk = 1024\n\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=self.audio_format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        self.get_logger().info('Audio capture started')\n        self.timer = self.create_timer(0.1, self.capture_audio)\n\n    def capture_audio(self):\n        try:\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\n            msg = AudioData()\n            msg.data = list(data)\n            self.publisher.publish(msg)\n        except Exception as e:\n            self.get_logger().error(f'Audio capture error: {str(e)}')\n\n    def __del__(self):\n        self.stream.stop_stream()\n        self.stream.close()\n        self.audio.terminate()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-whisper-integration",children:"Step 2: Whisper Integration"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Whisper Transcription Node:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import whisper\nimport torch\nfrom std_msgs.msg import String\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_transcription')\n\n        # Load Whisper model\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = whisper.load_model(\"base\", device=self.device)\n        self.get_logger().info(f'Whisper model loaded on {self.device}')\n\n        # Subscribers and publishers\n        self.audio_sub = self.create_subscription(\n            AudioData, 'audio/input', self.audio_callback, 10\n        )\n        self.text_pub = self.create_publisher(String, 'voice/transcription', 10)\n\n        # Audio buffer\n        self.audio_buffer = []\n        self.buffer_duration = 3.0  # seconds\n        self.sample_rate = 16000\n\n    def audio_callback(self, msg):\n        # Accumulate audio data\n        self.audio_buffer.extend(msg.data)\n\n        # Process when buffer is full\n        samples_needed = int(self.buffer_duration * self.sample_rate)\n        if len(self.audio_buffer) >= samples_needed:\n            self.process_audio()\n\n    def process_audio(self):\n        # Convert buffer to numpy array\n        audio_data = np.array(self.audio_buffer, dtype=np.int16)\n        audio_data = audio_data.astype(np.float32) / 32768.0  # Normalize\n\n        # Transcribe with Whisper\n        result = self.model.transcribe(\n            audio_data,\n            language='en',\n            task='transcribe',\n            fp16=(self.device == 'cuda')\n        )\n\n        transcription = result['text'].strip()\n\n        if transcription:\n            self.get_logger().info(f'Transcribed: {transcription}')\n            msg = String()\n            msg.data = transcription\n            self.text_pub.publish(msg)\n\n        # Clear buffer\n        self.audio_buffer = []\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-intent-recognition-and-command-parsing",children:"Step 3: Intent Recognition and Command Parsing"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Command Parser Node:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import re\nfrom robot_interfaces.msg import RobotCommand\n\nclass CommandParserNode(Node):\n    def __init__(self):\n        super().__init__('command_parser')\n\n        self.text_sub = self.create_subscription(\n            String, 'voice/transcription', self.parse_command, 10\n        )\n        self.command_pub = self.create_publisher(\n            RobotCommand, 'robot/command', 10\n        )\n\n        # Define command patterns\n        self.command_patterns = {\n            'navigate': [\n                r'go to (.*)',\n                r'move to (.*)',\n                r'navigate to (.*)'\n            ],\n            'grasp': [\n                r'pick up (.*)',\n                r'grab (.*)',\n                r'take (.*)'\n            ],\n            'place': [\n                r'put (.*) on (.*)',\n                r'place (.*) on (.*)',\n            ],\n            'follow': [\n                r'follow me',\n                r'come with me'\n            ],\n            'stop': [\n                r'stop',\n                r'halt',\n                r'freeze'\n            ]\n        }\n\n    def parse_command(self, msg):\n        text = msg.data.lower()\n        self.get_logger().info(f'Parsing: {text}')\n\n        # Match against patterns\n        for action, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.match(pattern, text)\n                if match:\n                    self.execute_action(action, match)\n                    return\n\n        self.get_logger().warn(f'Unknown command: {text}')\n\n    def execute_action(self, action, match):\n        cmd = RobotCommand()\n        cmd.action = action\n\n        if action == 'navigate':\n            cmd.target = match.group(1)\n        elif action in ['grasp', 'pick']:\n            cmd.object = match.group(1)\n        elif action == 'place':\n            cmd.object = match.group(1)\n            cmd.location = match.group(2)\n\n        self.command_pub.publish(cmd)\n        self.get_logger().info(f'Command: {action} -> {cmd}')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-connecting-to-robot-actions",children:"Step 4: Connecting to Robot Actions"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Action Executor Node:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from geometry_msgs.msg import PoseStamped\nfrom control_msgs.action import GripperCommand\nfrom rclpy.action import ActionClient\n\nclass VoiceActionExecutor(Node):\n    def __init__(self):\n        super().__init__('voice_action_executor')\n\n        self.command_sub = self.create_subscription(\n            RobotCommand, 'robot/command', self.handle_command, 10\n        )\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.gripper_client = ActionClient(self, GripperCommand, 'gripper_controller')\n\n    def handle_command(self, cmd):\n        if cmd.action == 'navigate':\n            self.navigate_to(cmd.target)\n        elif cmd.action == 'grasp':\n            self.grasp_object(cmd.object)\n        elif cmd.action == 'place':\n            self.place_object(cmd.location)\n        elif cmd.action == 'stop':\n            self.emergency_stop()\n\n    def navigate_to(self, target):\n        goal_msg = NavigateToPose.Goal()\n        # Convert target name to coordinates\n        goal_msg.pose = self.lookup_location(target)\n        self.nav_client.send_goal_async(goal_msg)\n        self.get_logger().info(f'Navigating to {target}')\n\n    def grasp_object(self, object_name):\n        # First, detect object location\n        object_pose = self.detect_object(object_name)\n\n        # Navigate to object\n        self.navigate_to(object_pose)\n\n        # Execute grasp\n        grasp_goal = GripperCommand.Goal()\n        grasp_goal.command.position = 0.0  # Closed\n        self.gripper_client.send_goal_async(grasp_goal)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,s.jsx)(n.h3,{id:"1-wake-word-detection",children:"1. Wake Word Detection"}),"\n",(0,s.jsx)(n.p,{children:'Implement always-listening mode with a wake word (e.g., "Hey Robot"):'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import pvporcupine\n\nclass WakeWordDetector(Node):\n    def __init__(self):\n        super().__init__('wake_word_detector')\n\n        # Initialize Porcupine wake word detector\n        self.porcupine = pvporcupine.create(\n            keywords=['hey-robot'],\n            sensitivities=[0.5]\n        )\n\n        self.listening = False\n\n    def detect_wake_word(self, audio_frame):\n        keyword_index = self.porcupine.process(audio_frame)\n\n        if keyword_index >= 0:\n            self.get_logger().info('Wake word detected!')\n            self.listening = True\n            # Start Whisper transcription\n            self.activate_transcription()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-multilingual-support",children:"2. Multilingual Support"}),"\n",(0,s.jsx)(n.p,{children:"Whisper supports 99 languages out of the box:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Automatic language detection\nresult = model.transcribe(audio, task='transcribe')\ndetected_language = result['language']\nself.get_logger().info(f'Detected language: {detected_language}')\n\n# Or specify language explicitly\nresult = model.transcribe(audio, language='es')  # Spanish\nresult = model.transcribe(audio, language='ur')  # Urdu\nresult = model.transcribe(audio, language='zh')  # Chinese\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-confidence-based-filtering",children:"3. Confidence-Based Filtering"}),"\n",(0,s.jsx)(n.p,{children:"Reject low-confidence transcriptions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def process_with_confidence(self, audio):\n    result = self.model.transcribe(\n        audio,\n        word_timestamps=True,\n        condition_on_previous_text=False\n    )\n\n    # Calculate average confidence\n    if 'segments' in result:\n        avg_confidence = sum(\n            seg.get('confidence', 0) for seg in result['segments']\n        ) / len(result['segments'])\n\n        if avg_confidence < 0.7:\n            self.get_logger().warn('Low confidence, ignoring')\n            return None\n\n    return result['text']\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-contextual-understanding-with-llms",children:"4. Contextual Understanding with LLMs"}),"\n",(0,s.jsx)(n.p,{children:"Enhance voice commands with LLM reasoning:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\n\nclass ContextualVoiceController(Node):\n    def __init__(self):\n        super().__init__(\'contextual_voice\')\n        self.conversation_history = []\n\n    def enhance_command(self, transcription):\n        # Add context from previous commands\n        prompt = f"""\n        Robot conversation history: {self.conversation_history}\n        User just said: "{transcription}"\n\n        Extract the robot action as JSON:\n        {{"action": "navigate|grasp|place", "target": "...", "parameters": {{...}}}}\n        """\n\n        response = openai.ChatCompletion.create(\n            model="gpt-4",\n            messages=[{"role": "user", "content": prompt}]\n        )\n\n        command = json.loads(response.choices[0].message.content)\n        return command\n'})}),"\n",(0,s.jsx)(n.h2,{id:"deployment-on-edge-devices",children:"Deployment on Edge Devices"}),"\n",(0,s.jsx)(n.h3,{id:"running-whisper-on-nvidia-jetson",children:"Running Whisper on NVIDIA Jetson"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Optimized Inference:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Use faster-whisper (optimized with CTranslate2)\nfrom faster_whisper import WhisperModel\n\nclass OptimizedWhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'optimized_whisper\')\n\n        # Load model with GPU acceleration\n        self.model = WhisperModel(\n            "base",\n            device="cuda",\n            compute_type="float16"  # Use half precision\n        )\n\n        self.get_logger().info(\'Optimized Whisper loaded\')\n\n    def transcribe(self, audio):\n        segments, info = self.model.transcribe(\n            audio,\n            beam_size=1,  # Faster decoding\n            language="en"\n        )\n\n        text = " ".join([segment.text for segment in segments])\n        return text\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance Benchmarks:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Device"}),(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Real-Time Factor"}),(0,s.jsx)(n.th,{children:"Latency"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Jetson Orin Nano"}),(0,s.jsx)(n.td,{children:"tiny"}),(0,s.jsx)(n.td,{children:"0.1x"}),(0,s.jsx)(n.td,{children:"~100ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Jetson Orin Nano"}),(0,s.jsx)(n.td,{children:"base"}),(0,s.jsx)(n.td,{children:"0.3x"}),(0,s.jsx)(n.td,{children:"~300ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Jetson AGX Orin"}),(0,s.jsx)(n.td,{children:"small"}),(0,s.jsx)(n.td,{children:"0.2x"}),(0,s.jsx)(n.td,{children:"~200ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"RTX 4090"}),(0,s.jsx)(n.td,{children:"large"}),(0,s.jsx)(n.td,{children:"0.05x"}),(0,s.jsx)(n.td,{children:"~50ms"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Real-Time Factor: < 1.0 means faster than real-time"})}),"\n",(0,s.jsx)(n.h2,{id:"handling-edge-cases",children:"Handling Edge Cases"}),"\n",(0,s.jsx)(n.h3,{id:"1-background-noise",children:"1. Background Noise"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Noise Filtering:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import noisereduce as nr\n\ndef filter_noise(self, audio, sample_rate):\n    # Reduce background noise\n    reduced_noise = nr.reduce_noise(\n        y=audio,\n        sr=sample_rate,\n        stationary=True,\n        prop_decrease=0.8\n    )\n    return reduced_noise\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-overlapping-speech",children:"2. Overlapping Speech"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Voice Activity Detection (VAD):"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from webrtcvad import Vad\n\nclass VoiceActivityNode(Node):\n    def __init__(self):\n        super().__init__('vad')\n        self.vad = Vad(3)  # Aggressiveness: 0-3\n\n    def is_speech(self, audio_frame, sample_rate):\n        return self.vad.is_speech(audio_frame, sample_rate)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-ambiguous-commands",children:"3. Ambiguous Commands"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Clarification Requests:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def handle_ambiguous_command(self, cmd):\n    if self.is_ambiguous(cmd):\n        self.speak("I\'m not sure what you meant. Could you clarify?")\n        # Wait for clarification\n        clarification = self.wait_for_response()\n        return self.resolve_with_context(cmd, clarification)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"1-emergency-stop",children:"1. Emergency Stop"}),"\n",(0,s.jsx)(n.p,{children:"Always implement voice-activated emergency stop:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"EMERGENCY_KEYWORDS = ['stop', 'halt', 'freeze', 'emergency']\n\ndef check_emergency_stop(self, transcription):\n    if any(keyword in transcription.lower() for keyword in EMERGENCY_KEYWORDS):\n        self.emergency_stop()\n        self.get_logger().warn('EMERGENCY STOP ACTIVATED')\n        return True\n    return False\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-command-confirmation",children:"2. Command Confirmation"}),"\n",(0,s.jsx)(n.p,{children:"For dangerous actions, require confirmation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def execute_dangerous_action(self, action):\n    self.speak(f"Are you sure you want me to {action}?")\n    confirmation = self.wait_for_confirmation(timeout=5.0)\n\n    if confirmation and \'yes\' in confirmation.lower():\n        self.execute(action)\n    else:\n        self.speak("Action cancelled")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"complete-end-to-end-example",children:"Complete End-to-End Example"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Full Voice-Controlled Navigation System:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport whisper\nimport pyaudio\nimport numpy as np\n\nclass VoiceNavigationRobot(Node):\n    def __init__(self):\n        super().__init__('voice_nav_robot')\n\n        # Whisper model\n        self.model = whisper.load_model(\"base\")\n\n        # Audio setup\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=16000,\n            input=True,\n            frames_per_buffer=1024\n        )\n\n        self.get_logger().info('Voice navigation ready')\n\n    def listen_and_execute(self):\n        self.get_logger().info('Listening for command...')\n\n        # Capture 3 seconds of audio\n        audio_buffer = []\n        for _ in range(int(16000 / 1024 * 3)):\n            data = self.stream.read(1024)\n            audio_buffer.extend(np.frombuffer(data, dtype=np.int16))\n\n        # Convert and normalize\n        audio_array = np.array(audio_buffer, dtype=np.float32) / 32768.0\n\n        # Transcribe\n        result = self.model.transcribe(audio_array)\n        command = result['text'].strip()\n        self.get_logger().info(f'Command: {command}')\n\n        # Execute\n        self.execute_command(command)\n\n    def execute_command(self, command):\n        cmd_lower = command.lower()\n\n        if 'forward' in cmd_lower:\n            self.move_forward()\n        elif 'backward' in cmd_lower:\n            self.move_backward()\n        elif 'left' in cmd_lower:\n            self.turn_left()\n        elif 'right' in cmd_lower:\n            self.turn_right()\n        elif 'stop' in cmd_lower:\n            self.stop()\n        else:\n            self.get_logger().warn(f'Unknown command: {command}')\n\ndef main():\n    rclpy.init()\n    robot = VoiceNavigationRobot()\n\n    try:\n        while rclpy.ok():\n            robot.listen_and_execute()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        robot.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization-tips",children:"Performance Optimization Tips"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use Smaller Models"}),": ",(0,s.jsx)(n.code,{children:"tiny"})," or ",(0,s.jsx)(n.code,{children:"base"})," for real-time applications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU Acceleration"}),": Always use CUDA when available"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Processing"}),": Process multiple audio chunks together"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caching"}),": Cache common phrases and responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Activity Detection"}),": Only process when speech is detected"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quantization"}),": Use INT8 quantization for edge devices"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whisper provides robust speech recognition"})," across languages and accents"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice control enables natural human-robot interaction"})," without keyboards or screens"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Edge deployment is feasible"})," with model optimization (faster-whisper, quantization)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety features are critical"})," (emergency stop, command confirmation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Combining Whisper with LLMs"})," enables contextual understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time performance requires"})," careful model selection and optimization"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives-achieved",children:"Learning Objectives Achieved"}),"\n",(0,s.jsx)(n.p,{children:"By completing this chapter, you should be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate OpenAI Whisper with ROS 2 for speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Build a complete voice-to-action pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Implement command parsing and intent recognition"}),"\n",(0,s.jsx)(n.li,{children:"Deploy voice control on edge devices (Jetson)"}),"\n",(0,s.jsx)(n.li,{children:"Handle edge cases (noise, ambiguity, overlapping speech)"}),"\n",(0,s.jsx)(n.li,{children:"Design safe voice-controlled robot systems"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": We'll explore cognitive planning with LLMs, translating natural language instructions into complex multi-step ROS 2 action sequences."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);