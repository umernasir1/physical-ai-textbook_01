"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[1646],{5899:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4-vla/capstone-autonomous-humanoid","title":"Capstone Project (The Autonomous Humanoid)","description":"Project Overview","source":"@site/docs/module4-vla/capstone-autonomous-humanoid.md","sourceDirName":"module4-vla","slug":"/module4-vla/capstone-autonomous-humanoid","permalink":"/-physical-ai-textbook_01/docs/module4-vla/capstone-autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/umernasir1/physical-ai-textbook/tree/main/docs/module4-vla/capstone-autonomous-humanoid.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Capstone Project (The Autonomous Humanoid)"},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning (Translating Natural Language to ROS 2 Actions)","permalink":"/-physical-ai-textbook_01/docs/module4-vla/cognitive-planning-ros2-actions"},"next":{"title":"Create a Document","permalink":"/-physical-ai-textbook_01/docs/tutorial-basics/create-a-document"}}');var t=i(4848),a=i(8453);const s={sidebar_position:4,title:"Capstone Project (The Autonomous Humanoid)"},r="Capstone Project: The Autonomous Humanoid",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Phase 1: Project Setup",id:"phase-1-project-setup",level:2},{value:"Create Workspace",id:"create-workspace",level:3},{value:"Directory Structure",id:"directory-structure",level:3},{value:"Phase 2: Robot Model",id:"phase-2-robot-model",level:2},{value:"URDF Humanoid Definition",id:"urdf-humanoid-definition",level:3},{value:"Phase 3: High-Level Task Planner",id:"phase-3-high-level-task-planner",level:2},{value:"Voice-to-Task Pipeline",id:"voice-to-task-pipeline",level:3},{value:"Phase 4: Object Detection and Localization",id:"phase-4-object-detection-and-localization",level:2},{value:"Phase 5: Grasp Planning",id:"phase-5-grasp-planning",level:2},{value:"Phase 6: Behavior Tree Integration",id:"phase-6-behavior-tree-integration",level:2},{value:"Complete Task Execution",id:"complete-task-execution",level:3},{value:"Phase 7: Testing and Validation",id:"phase-7-testing-and-validation",level:2},{value:"Launch Everything",id:"launch-everything",level:3},{value:"Run the Capstone",id:"run-the-capstone",level:3},{value:"Expected Behavior",id:"expected-behavior",level:2},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Extensions and Improvements",id:"extensions-and-improvements",level:2},{value:"Add More Capabilities",id:"add-more-capabilities",level:3},{value:"Real Hardware Deployment",id:"real-hardware-deployment",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"})}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.p,{children:"This capstone project synthesizes all previous modules into a fully autonomous humanoid robot capable of:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),": Using LiDAR, RGB-D cameras, and IMU for environmental awareness"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Localization"}),": VSLAM for knowing where it is"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation"}),": Nav2 for planning paths and avoiding obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulation"}),": Grasping objects using visual servoing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intelligence"}),": Natural language understanding for task commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interaction"}),": Communicating with humans through speech and gestures"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scenario:"})," A humanoid butler robot that can:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Receive voice commands ("Bring me a water bottle")'}),"\n",(0,t.jsx)(n.li,{children:"Navigate autonomously to the kitchen"}),"\n",(0,t.jsx)(n.li,{children:"Find and grasp the bottle"}),"\n",(0,t.jsx)(n.li,{children:"Return and hand it to the human"}),"\n",(0,t.jsx)(n.li,{children:"Report status via speech"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Voice Command (Whisper) \u2192 Task Parser (GPT-4) \u2192 Task Execution\n                                                        \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 High-Level Planner (behavior tree)                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Navigation (Nav2) \u2190\u2192 Manipulation (MoveIt2) \u2190\u2192 Perception   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Localization (VSLAM) \u2190\u2192 Object Detection (YOLO) \u2190\u2192 Grasping \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Low-Level Control (Joint Controllers) \u2190\u2192 Balance (Whole-Body)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n                    Simulation (Gazebo/Isaac)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"phase-1-project-setup",children:"Phase 1: Project Setup"}),"\n",(0,t.jsx)(n.h3,{id:"create-workspace",children:"Create Workspace"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/humanoid_ws/src\ncd ~/humanoid_ws/src\n\n# Clone dependencies\ngit clone https://github.com/ros-planning/moveit2.git -b humble\ngit clone https://github.com/ros-planning/navigation2.git -b humble\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\n\n# Create our package\nros2 pkg create humanoid_butler --build-type ament_python --dependencies rclpy std_msgs geometry_msgs sensor_msgs nav_msgs\n\ncd ~/humanoid_ws\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,t.jsx)(n.h3,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"humanoid_butler/\n\u251c\u2500\u2500 launch/          # Launch files\n\u251c\u2500\u2500 config/          # Parameter files (Nav2, VSLAM)\n\u251c\u2500\u2500 urdf/            # Robot description\n\u251c\u2500\u2500 worlds/          # Gazebo environments\n\u251c\u2500\u2500 models/          # 3D models for objects\n\u251c\u2500\u2500 scripts/         # Python nodes\n\u2502   \u251c\u2500\u2500 task_planner.py\n\u2502   \u251c\u2500\u2500 object_detector.py\n\u2502   \u251c\u2500\u2500 grasp_planner.py\n\u2502   \u2514\u2500\u2500 speech_interface.py\n\u2514\u2500\u2500 behavior_trees/  # BT XML files\n"})}),"\n",(0,t.jsx)(n.h2,{id:"phase-2-robot-model",children:"Phase 2: Robot Model"}),"\n",(0,t.jsx)(n.h3,{id:"urdf-humanoid-definition",children:"URDF Humanoid Definition"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- urdf/humanoid.urdf.xacro --\x3e\n<?xml version="1.0"?>\n<robot name="humanoid_butler" xmlns:xacro="http://www.ros.org/wiki/xacro">\n\n  \x3c!-- Base link (torso) --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.3 0.2 0.6"/>  \x3c!-- 30cm x 20cm x 60cm torso --\x3e\n      </geometry>\n      <material name="grey">\n        <color rgba="0.5 0.5 0.5 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.3 0.2 0.6"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="15.0"/>\n      <inertia ixx="0.5" ixy="0" ixz="0" iyy="0.5" iyz="0" izz="0.3"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Head with camera --\x3e\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.12"/>\n      </geometry>\n    </visual>\n    <inertial>\n      <mass value="2.0"/>\n      <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <joint name="neck" type="revolute">\n    <parent link="base_link"/>\n    <child link="head"/>\n    <origin xyz="0 0 0.35" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n    <limit lower="-1.57" upper="1.57" effort="10" velocity="1.0"/>\n  </joint>\n\n  \x3c!-- Left Arm (simplified 3-DOF) --\x3e\n  <xacro:macro name="arm" params="prefix reflect">\n    <link name="${prefix}_shoulder">\n      <visual>\n        <geometry>\n          <cylinder radius="0.05" length="0.1"/>\n        </geometry>\n      </visual>\n      <inertial>\n        <mass value="1.0"/>\n        <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.01"/>\n      </inertial>\n    </link>\n\n    <joint name="${prefix}_shoulder_joint" type="revolute">\n      <parent link="base_link"/>\n      <child link="${prefix}_shoulder"/>\n      <origin xyz="0 ${reflect*0.15} 0.25" rpy="0 0 0"/>\n      <axis xyz="1 0 0"/>\n      <limit lower="-3.14" upper="3.14" effort="30" velocity="2.0"/>\n    </joint>\n\n    <link name="${prefix}_upper_arm">\n      <visual>\n        <geometry>\n          <cylinder radius="0.04" length="0.3"/>\n        </geometry>\n      </visual>\n      <inertial>\n        <mass value="1.5"/>\n        <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.01"/>\n      </inertial>\n    </link>\n\n    <joint name="${prefix}_elbow_joint" type="revolute">\n      <parent link="${prefix}_shoulder"/>\n      <child link="${prefix}_upper_arm"/>\n      <origin xyz="0 0 -0.15" rpy="0 0 0"/>\n      <axis xyz="0 1 0"/>\n      <limit lower="0" upper="2.5" effort="20" velocity="2.0"/>\n    </joint>\n\n    <link name="${prefix}_gripper">\n      <visual>\n        <geometry>\n          <box size="0.08 0.08 0.12"/>\n        </geometry>\n      </visual>\n      <inertial>\n        <mass value="0.5"/>\n        <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>\n      </inertial>\n    </link>\n\n    <joint name="${prefix}_wrist_joint" type="fixed">\n      <parent link="${prefix}_upper_arm"/>\n      <child link="${prefix}_gripper"/>\n      <origin xyz="0 0 -0.15" rpy="0 0 0"/>\n    </joint>\n  </xacro:macro>\n\n  \x3c!-- Instantiate both arms --\x3e\n  <xacro:arm prefix="left" reflect="1"/>\n  <xacro:arm prefix="right" reflect="-1"/>\n\n  \x3c!-- Sensors --\x3e\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.1 0.05"/>\n      </geometry>\n    </visual>\n  </link>\n\n  <joint name="camera_joint" type="fixed">\n    <parent link="head"/>\n    <child link="camera_link"/>\n    <origin xyz="0.1 0 0" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugins --\x3e\n  <gazebo reference="camera_link">\n    <sensor name="camera" type="camera">\n      <update_rate>30</update_rate>\n      <camera>\n        <horizontal_fov>1.3962634</horizontal_fov>\n        <image>\n          <width>1280</width>\n          <height>720</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>100</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <ros>\n          <namespace>/humanoid</namespace>\n          <remapping>image_raw:=camera/image_raw</remapping>\n        </ros>\n        <camera_name>camera</camera_name>\n        <frame_name>camera_link</frame_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n</robot>\n'})}),"\n",(0,t.jsx)(n.h2,{id:"phase-3-high-level-task-planner",children:"Phase 3: High-Level Task Planner"}),"\n",(0,t.jsx)(n.h3,{id:"voice-to-task-pipeline",children:"Voice-to-Task Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\nimport os\n\nclass TaskPlanner(Node):\n    """Convert natural language commands to robot tasks"""\n\n    def __init__(self):\n        super().__init__(\'task_planner\')\n\n        # Subscribe to voice commands\n        self.voice_sub = self.create_subscription(\n            String,\n            \'/voice_command\',\n            self.voice_callback,\n            10\n        )\n\n        # Publish structured tasks\n        self.task_pub = self.create_publisher(String, \'/robot_task\', 10)\n\n        # OpenAI API\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\n\n    def voice_callback(self, msg):\n        """Parse voice command into structured task"""\n\n        user_command = msg.data\n        self.get_logger().info(f\'Received command: {user_command}\')\n\n        # Use GPT-4 to parse intent\n        system_prompt = """You are a robot task parser. Convert natural language commands into structured JSON tasks.\n\nAvailable actions: navigate, grasp, place, speak\nAvailable objects: water_bottle, coffee_cup, book, remote\nAvailable locations: kitchen, living_room, bedroom, table\n\nOutput format:\n{\n  "action": "grasp",\n  "object": "water_bottle",\n  "location": "kitchen"\n}\n"""\n\n        response = openai.ChatCompletion.create(\n            model="gpt-4",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_command}\n            ],\n            temperature=0.0\n        )\n\n        task_json = response.choices[0].message.content\n\n        # Publish task\n        task_msg = String()\n        task_msg.data = task_json\n        self.task_pub.publish(task_msg)\n\n        self.get_logger().info(f\'Task: {task_json}\')\n\ndef main():\n    rclpy.init()\n    node = TaskPlanner()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"phase-4-object-detection-and-localization",children:"Phase 4: Object Detection and Localization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n\nclass ObjectDetector(Node):\n    def __init__(self):\n        super().__init__('object_detector')\n\n        self.bridge = CvBridge()\n\n        # YOLO model\n        self.model = YOLO('yolov8n.pt')  # Nano model for speed\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/humanoid/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publish object poses\n        self.object_pub = self.create_publisher(\n            PoseStamped,\n            '/detected_objects',\n            10\n        )\n\n        # Target object\n        self.target_object = 'bottle'\n\n    def image_callback(self, msg):\n        \"\"\"Detect objects in camera image\"\"\"\n\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Run YOLO detection\n        results = self.model(cv_image)\n\n        for result in results:\n            boxes = result.boxes\n\n            for box in boxes:\n                # Get class name\n                class_id = int(box.cls[0])\n                class_name = self.model.names[class_id]\n\n                if class_name == self.target_object:\n                    # Get bounding box\n                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n\n                    # Calculate 3D position (simplified - assumes known object size)\n                    center_x = (x1 + x2) / 2\n                    center_y = (y1 + y2) / 2\n\n                    # Estimate distance from bounding box size\n                    box_height = y2 - y1\n                    known_height = 0.25  # 25cm bottle\n                    focal_length = 600  # Camera intrinsic\n                    distance = (known_height * focal_length) / box_height\n\n                    # Publish object pose\n                    pose = PoseStamped()\n                    pose.header = msg.header\n                    pose.header.frame_id = 'camera_link'\n\n                    # Convert pixel coordinates to 3D (assuming camera calibration)\n                    pose.pose.position.x = distance\n                    pose.pose.position.y = -(center_x - 640) * distance / focal_length\n                    pose.pose.position.z = -(center_y - 360) * distance / focal_length\n\n                    pose.pose.orientation.w = 1.0\n\n                    self.object_pub.publish(pose)\n\n                    self.get_logger().info(\n                        f'Detected {class_name} at ({pose.pose.position.x:.2f}, '\n                        f'{pose.pose.position.y:.2f}, {pose.pose.position.z:.2f})'\n                    )\n\n                    # Visualize\n                    cv2.rectangle(cv_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n                    cv2.putText(cv_image, class_name, (int(x1), int(y1)-10),\n                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        cv2.imshow('Object Detection', cv_image)\n        cv2.waitKey(1)\n\ndef main():\n    rclpy.init()\n    node = ObjectDetector()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"phase-5-grasp-planning",children:"Phase 5: Grasp Planning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom moveit_msgs.msg import MoveGroupAction\nfrom rclpy.action import ActionClient\n\nclass GraspPlanner(Node):\n    def __init__(self):\n        super().__init__(\'grasp_planner\')\n\n        # Subscribe to detected objects\n        self.object_sub = self.create_subscription(\n            PoseStamped,\n            \'/detected_objects\',\n            self.object_callback,\n            10\n        )\n\n        # MoveIt2 action client\n        self.moveit_client = ActionClient(\n            self,\n            MoveGroupAction,\n            \'/move_action\'\n        )\n\n    def object_callback(self, object_pose):\n        """Plan grasp for detected object"""\n\n        self.get_logger().info(\'Planning grasp...\')\n\n        # Calculate pre-grasp pose (approach from above)\n        pre_grasp = PoseStamped()\n        pre_grasp.header = object_pose.header\n        pre_grasp.pose.position.x = object_pose.pose.position.x\n        pre_grasp.pose.position.y = object_pose.pose.position.y\n        pre_grasp.pose.position.z = object_pose.pose.position.z + 0.15  # 15cm above\n\n        # Orientation: gripper pointing down\n        pre_grasp.pose.orientation.x = 0.707\n        pre_grasp.pose.orientation.y = 0.707\n        pre_grasp.pose.orientation.z = 0.0\n        pre_grasp.pose.orientation.w = 0.0\n\n        # Move to pre-grasp\n        self.move_arm_to_pose(pre_grasp)\n\n        # Move to grasp\n        grasp_pose = object_pose\n        grasp_pose.pose.position.z += 0.05  # Slight offset\n        self.move_arm_to_pose(grasp_pose)\n\n        # Close gripper\n        self.close_gripper()\n\n        # Lift object\n        lift_pose = grasp_pose\n        lift_pose.pose.position.z += 0.2\n        self.move_arm_to_pose(lift_pose)\n\n        self.get_logger().info(\'Grasp complete!\')\n\n    def move_arm_to_pose(self, target_pose):\n        """Send MoveIt2 goal"""\n        # Simplified - actual implementation uses MoveGroupInterface\n        pass\n\n    def close_gripper(self):\n        """Close gripper fingers"""\n        # Publish gripper command\n        pass\n\ndef main():\n    rclpy.init()\n    node = GraspPlanner()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"phase-6-behavior-tree-integration",children:"Phase 6: Behavior Tree Integration"}),"\n",(0,t.jsx)(n.h3,{id:"complete-task-execution",children:"Complete Task Execution"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- behavior_trees/butler_task.xml --\x3e\n<root main_tree_to_execute="ButlerTask">\n  <BehaviorTree ID="ButlerTask">\n    <Sequence name="fetch_object">\n\n      \x3c!-- Parse voice command --\x3e\n      <Action ID="ParseVoiceCommand" command="{voice_input}" task="{task}"/>\n\n      \x3c!-- Navigate to object location --\x3e\n      <Action ID="NavigateToLocation" location="{task.location}"/>\n\n      \x3c!-- Search for object --\x3e\n      <Action ID="DetectObject" object_name="{task.object}" object_pose="{object_pose}"/>\n\n      \x3c!-- Plan and execute grasp --\x3e\n      <Action ID="GraspObject" pose="{object_pose}"/>\n\n      \x3c!-- Return to human --\x3e\n      <Action ID="NavigateToLocation" location="living_room"/>\n\n      \x3c!-- Hand over object --\x3e\n      <Action ID="HandOver"/>\n\n      \x3c!-- Speak confirmation --\x3e\n      <Action ID="Speak" text="Here is your {task.object}"/>\n\n    </Sequence>\n  </BehaviorTree>\n</root>\n'})}),"\n",(0,t.jsx)(n.h2,{id:"phase-7-testing-and-validation",children:"Phase 7: Testing and Validation"}),"\n",(0,t.jsx)(n.h3,{id:"launch-everything",children:"Launch Everything"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# launch/capstone_complete.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Gazebo simulation\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource([\n                get_package_share_directory('gazebo_ros'),\n                '/launch/gazebo.launch.py'\n            ])\n        ),\n\n        # Spawn robot\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=['-entity', 'humanoid', '-file', 'urdf/humanoid.urdf']\n        ),\n\n        # VSLAM\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='isaac_ros_visual_slam'\n        ),\n\n        # Nav2\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource([\n                get_package_share_directory('nav2_bringup'),\n                '/launch/navigation_launch.py'\n            ])\n        ),\n\n        # Task planner\n        Node(\n            package='humanoid_butler',\n            executable='task_planner'\n        ),\n\n        # Object detector\n        Node(\n            package='humanoid_butler',\n            executable='object_detector'\n        ),\n\n        # Grasp planner\n        Node(\n            package='humanoid_butler',\n            executable='grasp_planner'\n        ),\n\n        # Behavior tree\n        Node(\n            package='nav2_bt_navigator',\n            executable='bt_navigator',\n            parameters=[{'default_bt_xml_filename': 'behavior_trees/butler_task.xml'}]\n        ),\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"run-the-capstone",children:"Run the Capstone"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch simulation\nros2 launch humanoid_butler capstone_complete.launch.py\n\n# Terminal 2: Send voice command\nros2 topic pub /voice_command std_msgs/String \"data: 'Bring me a water bottle from the kitchen'\"\n\n# Watch the magic happen!\n"})}),"\n",(0,t.jsx)(n.h2,{id:"expected-behavior",children:"Expected Behavior"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Recognition"}),': Robot hears "Bring me a water bottle from the kitchen"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Parsing"}),": GPT-4 extracts: action=grasp, object=water_bottle, location=kitchen"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation"}),": Nav2 plans path to kitchen, humanoid walks there"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),": Camera detects water bottle using YOLO"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasping"}),": MoveIt2 plans arm trajectory, gripper closes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Return"}),": Nav2 plans return path to human"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handoff"}),": Extends arm toward human"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech"}),': "Here is your water bottle"']}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Metric"}),(0,t.jsx)(n.th,{children:"Target"}),(0,t.jsx)(n.th,{children:"How to Measure"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Task Success Rate"})}),(0,t.jsx)(n.td,{children:">80%"}),(0,t.jsx)(n.td,{children:"Complete fetch tasks / Total attempts"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Navigation Time"})}),(0,t.jsx)(n.td,{children:"<60s"}),(0,t.jsx)(n.td,{children:"Time from command to reaching object"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Grasp Success"})}),(0,t.jsx)(n.td,{children:">70%"}),(0,t.jsx)(n.td,{children:"Successful grasps / Attempts"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Localization Error"})}),(0,t.jsx)(n.td,{children:"<10cm"}),(0,t.jsx)(n.td,{children:"VSLAM drift after 100m"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Safety"})}),(0,t.jsx)(n.td,{children:"0 collisions"}),(0,t.jsx)(n.td,{children:"Obstacle collisions during nav"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"extensions-and-improvements",children:"Extensions and Improvements"}),"\n",(0,t.jsx)(n.h3,{id:"add-more-capabilities",children:"Add More Capabilities"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Object Tasks"}),': "Bring me a cup AND a book"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conditional Logic"}),': "If there\'s no coffee, bring tea"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning"}),": Improve grasping from failures"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human Following"}),": Walk alongside a person"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emotional Expression"}),": Facial animations for engagement"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"real-hardware-deployment",children:"Real Hardware Deployment"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Replace Gazebo with real sensors"}),"\n",(0,t.jsx)(n.li,{children:"Calibrate camera intrinsics"}),"\n",(0,t.jsx)(n.li,{children:"Tune gait parameters for stability"}),"\n",(0,t.jsx)(n.li,{children:"Add safety e-stops"}),"\n",(0,t.jsx)(n.li,{children:"Test in real environments"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This capstone project demonstrates the complete robotics stack:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Modules Integrated:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Module 1: ROS 2 for communication"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Module 2: Gazebo for simulation, Unity for rendering"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Module 3: Isaac ROS for perception, Nav2 for navigation"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Module 4: Whisper for voice, GPT-4 for reasoning"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Achievements:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"End-to-end autonomous behavior"}),"\n",(0,t.jsx)(n.li,{children:"Multi-modal perception (vision + LiDAR + IMU)"}),"\n",(0,t.jsx)(n.li,{children:"Natural language understanding"}),"\n",(0,t.jsx)(n.li,{children:"Robust navigation and manipulation"}),"\n",(0,t.jsx)(n.li,{children:"Human-robot collaboration"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Congratulations!"})," You've built a fully autonomous humanoid robot from scratch. This foundation prepares you for cutting-edge research and industry applications in physical AI."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Next Steps:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Deploy to real humanoid hardware (e.g., Unitree H1, Boston Dynamics Atlas)"}),"\n",(0,t.jsx)(n.li,{children:"Contribute to open-source robotics projects"}),"\n",(0,t.jsx)(n.li,{children:"Research advanced topics: reinforcement learning for locomotion, sim-to-real transfer, multi-robot coordination"}),"\n",(0,t.jsx)(n.li,{children:"Build the future of physical AI!"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The era of humanoid robots in everyday life is just beginning\u2014and you're ready to be part of it."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>r});var o=i(6540);const t={},a=o.createContext(t);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);