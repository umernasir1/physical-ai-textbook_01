"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[4181],{1855:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module2-digital-twin/high-fidelity-rendering-human-robot-interaction","title":"High-fidelity Rendering and Human-Robot Interaction","description":"Introduction","source":"@site/docs/module2-digital-twin/high-fidelity-rendering-human-robot-interaction.md","sourceDirName":"module2-digital-twin","slug":"/module2-digital-twin/high-fidelity-rendering-human-robot-interaction","permalink":"/physical-ai-textbook_01/docs/module2-digital-twin/high-fidelity-rendering-human-robot-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/umernasir1/physical-ai-textbook/tree/main/docs/module2-digital-twin/high-fidelity-rendering-human-robot-interaction.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"High-fidelity Rendering and Human-Robot Interaction"},"sidebar":"tutorialSidebar","previous":{"title":"Physics Simulation and Environment Building","permalink":"/physical-ai-textbook_01/docs/module2-digital-twin/physics-simulation-environment-building"},"next":{"title":"Simulating Sensors (LIDAR, Depth Cameras, IMUs)","permalink":"/physical-ai-textbook_01/docs/module2-digital-twin/simulating-sensors"}}');var r=i(4848),o=i(8453);const a={sidebar_position:2,title:"High-fidelity Rendering and Human-Robot Interaction"},s="High-fidelity Rendering and Human-Robot Interaction",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"The Rendering Pipeline for Robotics",id:"the-rendering-pipeline-for-robotics",level:2},{value:"Understanding the Graphics Pipeline",id:"understanding-the-graphics-pipeline",level:3},{value:"Choosing HDRP for Humanoid Simulation",id:"choosing-hdrp-for-humanoid-simulation",level:3},{value:"Materials and Lighting for Realistic Robots",id:"materials-and-lighting-for-realistic-robots",level:2},{value:"Physically-Based Materials (PBR)",id:"physically-based-materials-pbr",level:3},{value:"Advanced Lighting Techniques",id:"advanced-lighting-techniques",level:3},{value:"Human-Robot Interaction (HRI) Fundamentals",id:"human-robot-interaction-hri-fundamentals",level:2},{value:"The HRI Design Space",id:"the-hri-design-space",level:3},{value:"Implementing Gaze Behavior",id:"implementing-gaze-behavior",level:3},{value:"Proxemics: Respecting Personal Space",id:"proxemics-respecting-personal-space",level:3},{value:"Gesture Recognition and Generation",id:"gesture-recognition-and-generation",level:2},{value:"Recognizing Human Gestures",id:"recognizing-human-gestures",level:3},{value:"Generating Expressive Robot Gestures",id:"generating-expressive-robot-gestures",level:3},{value:"Natural Language Interaction",id:"natural-language-interaction",level:2},{value:"Speech Synthesis Integration",id:"speech-synthesis-integration",level:3},{value:"VR/AR Integration for Human-Robot Testing",id:"vrar-integration-for-human-robot-testing",level:2},{value:"Testing HRI in Virtual Reality",id:"testing-hri-in-virtual-reality",level:3},{value:"Performance Optimization for Real-Time Interaction",id:"performance-optimization-for-real-time-interaction",level:2},{value:"Rendering Optimizations",id:"rendering-optimizations",level:3},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"high-fidelity-rendering-and-human-robot-interaction",children:"High-fidelity Rendering and Human-Robot Interaction"})}),"\n",(0,r.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(e.p,{children:["While Gazebo excels at physics simulation, ",(0,r.jsx)(e.strong,{children:"Unity"})," offers photorealistic rendering and advanced interaction capabilities essential for developing humanoid robots that work alongside humans. This chapter explores how to leverage Unity's powerful graphics engine and interaction systems to create compelling digital twins that bridge simulation and reality."]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Why Unity for Humanoid Robotics?"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Photorealistic Graphics"}),": Ray tracing, global illumination, and material systems that mirror real-world lighting"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Human Animation"}),": Mecanim animation system for natural human motion"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cross-Platform"}),": Deploy simulations to VR/AR for immersive testing"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Asset Ecosystem"}),": Massive library of 3D models, environments, and animations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ML-Agents"}),": Native reinforcement learning framework for training AI"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"the-rendering-pipeline-for-robotics",children:"The Rendering Pipeline for Robotics"}),"\n",(0,r.jsx)(e.h3,{id:"understanding-the-graphics-pipeline",children:"Understanding the Graphics Pipeline"}),"\n",(0,r.jsx)(e.p,{children:"Unity's rendering pipeline consists of several stages that affect how realistic your simulation appears:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Scene \u2192 Culling \u2192 Rendering \u2192 Post-Processing \u2192 Display\n"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Key Rendering Pipelines:"})}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Pipeline"}),(0,r.jsx)(e.th,{children:"Use Case"}),(0,r.jsx)(e.th,{children:"Performance"}),(0,r.jsx)(e.th,{children:"Visual Quality"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Built-in"})}),(0,r.jsx)(e.td,{children:"Legacy, simple scenes"}),(0,r.jsx)(e.td,{children:"High"}),(0,r.jsx)(e.td,{children:"Low-Medium"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Universal (URP)"})}),(0,r.jsx)(e.td,{children:"Mobile, VR, general robotics"}),(0,r.jsx)(e.td,{children:"Medium-High"}),(0,r.jsx)(e.td,{children:"Medium-High"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"High Definition (HDRP)"})}),(0,r.jsx)(e.td,{children:"Photorealistic training data"}),(0,r.jsx)(e.td,{children:"Low-Medium"}),(0,r.jsx)(e.td,{children:"Very High"})]})]})]}),"\n",(0,r.jsx)(e.h3,{id:"choosing-hdrp-for-humanoid-simulation",children:"Choosing HDRP for Humanoid Simulation"}),"\n",(0,r.jsxs)(e.p,{children:["For humanoid robotics, ",(0,r.jsx)(e.strong,{children:"HDRP (High Definition Render Pipeline)"})," is recommended because:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Physically-Based Materials"}),": Accurate surface properties (metal, skin, fabric)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Volumetric Lighting"}),": Realistic fog, light shafts for depth perception"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Ray-Traced Reflections"}),": Critical for mirror-like surfaces and metallic robots"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Advanced Shadows"}),": Soft, contact-hardening shadows for visual realism"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"// Example: Setting up HDRP in a ROS-Unity bridge scene\nusing UnityEngine;\nusing UnityEngine.Rendering.HighDefinition;\n\npublic class HDRPSetup : MonoBehaviour\n{\n    void Start()\n    {\n        // Configure HDRP volume for robotics simulation\n        Volume volume = gameObject.AddComponent<Volume>();\n        volume.isGlobal = true;\n\n        VolumeProfile profile = volume.sharedProfile;\n\n        // Enable ambient occlusion for depth cues\n        if (profile.TryGet(out AmbientOcclusion ao))\n        {\n            ao.intensity.value = 0.5f;\n            ao.directLightingStrength.value = 0.25f;\n        }\n\n        // Configure exposure for consistent lighting\n        if (profile.TryGet(out Exposure exposure))\n        {\n            exposure.mode.value = ExposureMode.Fixed;\n            exposure.fixedExposure.value = 12f;\n        }\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"materials-and-lighting-for-realistic-robots",children:"Materials and Lighting for Realistic Robots"}),"\n",(0,r.jsx)(e.h3,{id:"physically-based-materials-pbr",children:"Physically-Based Materials (PBR)"}),"\n",(0,r.jsx)(e.p,{children:"Humanoid robots contain diverse materials: aluminum frames, rubber grips, plastic casings, LED displays. Unity's PBR system models these accurately."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Key Material Properties:"})}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Albedo"})," (Base Color): The diffuse color of the surface"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Metallic"}),": Whether the surface is metallic (0 = dielectric, 1 = metal)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Smoothness"}),": Surface roughness (0 = rough, 1 = mirror-smooth)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Normal Map"}),": Surface detail without adding geometry"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Emission"}),": Self-illuminating surfaces (LEDs, screens)"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'// Creating a robot metal material programmatically\nMaterial CreateRobotMetal()\n{\n    Material metal = new Material(Shader.Find("HDRP/Lit"));\n\n    // Brushed aluminum properties\n    metal.SetColor("_BaseColor", new Color(0.9f, 0.9f, 0.9f));\n    metal.SetFloat("_Metallic", 1.0f);        // Fully metallic\n    metal.SetFloat("_Smoothness", 0.7f);      // Slightly rough\n    metal.SetFloat("_NormalScale", 0.5f);     // Subtle surface detail\n\n    return metal;\n}\n\n// Soft rubber grip material\nMaterial CreateRubberGrip()\n{\n    Material rubber = new Material(Shader.Find("HDRP/Lit"));\n\n    rubber.SetColor("_BaseColor", new Color(0.1f, 0.1f, 0.1f));\n    rubber.SetFloat("_Metallic", 0.0f);       // Non-metallic\n    rubber.SetFloat("_Smoothness", 0.2f);     // Very rough\n\n    return rubber;\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"advanced-lighting-techniques",children:"Advanced Lighting Techniques"}),"\n",(0,r.jsx)(e.p,{children:"Proper lighting is essential for computer vision systems to work in simulation."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Three-Point Lighting Setup:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'public class RoboticsLightingRig : MonoBehaviour\n{\n    public Transform robot;\n\n    void CreateThreePointLighting()\n    {\n        // Key Light (main illumination)\n        GameObject keyLight = new GameObject("Key Light");\n        HDAdditionalLightData keyLightData = keyLight.AddComponent<Light>().gameObject.AddComponent<HDAdditionalLightData>();\n        keyLight.transform.position = robot.position + new Vector3(2, 3, 2);\n        keyLight.transform.LookAt(robot);\n        keyLightData.intensity = 50000;\n        keyLightData.SetColor(Color.white);\n\n        // Fill Light (soften shadows)\n        GameObject fillLight = new GameObject("Fill Light");\n        HDAdditionalLightData fillLightData = fillLight.AddComponent<Light>().gameObject.AddComponent<HDAdditionalLightData>();\n        fillLight.transform.position = robot.position + new Vector3(-2, 1, 1);\n        fillLight.transform.LookAt(robot);\n        fillLightData.intensity = 20000;\n\n        // Back Light (separation from background)\n        GameObject backLight = new GameObject("Back Light");\n        HDAdditionalLightData backLightData = backLight.AddComponent<Light>().gameObject.AddComponent<HDAdditionalLightData>();\n        backLight.transform.position = robot.position + new Vector3(0, 2, -2);\n        backLight.transform.LookAt(robot);\n        backLightData.intensity = 30000;\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"human-robot-interaction-hri-fundamentals",children:"Human-Robot Interaction (HRI) Fundamentals"}),"\n",(0,r.jsx)(e.h3,{id:"the-hri-design-space",children:"The HRI Design Space"}),"\n",(0,r.jsx)(e.p,{children:"Human-Robot Interaction encompasses:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Physical Interaction"}),": Handshakes, object handoffs, collaborative manipulation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Social Interaction"}),": Gaze, gestures, proxemics (personal space)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cognitive Interaction"}),": Intent recognition, task understanding, shared goals"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Key HRI Principles for Humanoids:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Predictability"}),": Humans should understand robot intentions"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Legibility"}),": Robot motions should clearly communicate goals"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Comfort"}),": Respect personal space and move naturally"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Safety"}),": Never surprise or threaten users"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"implementing-gaze-behavior",children:"Implementing Gaze Behavior"}),"\n",(0,r.jsx)(e.p,{children:"Eye contact is fundamental to human communication. Humanoid robots must simulate natural gaze patterns."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\n\npublic class HumanoidGaze : MonoBehaviour\n{\n    public Transform headBone;\n    public Transform leftEye;\n    public Transform rightEye;\n    public Transform currentTarget;\n\n    [Range(0f, 1f)]\n    public float blinkFrequency = 0.1f;\n\n    private float nextBlinkTime;\n    private float gazeShiftTimer;\n\n    void Update()\n    {\n        // Natural head tracking with damping\n        if (currentTarget != null)\n        {\n            Vector3 targetDirection = currentTarget.position - headBone.position;\n            Quaternion targetRotation = Quaternion.LookRotation(targetDirection);\n\n            // Smooth head movement (humans don't snap their heads)\n            headBone.rotation = Quaternion.Slerp(\n                headBone.rotation,\n                targetRotation,\n                Time.deltaTime * 2f\n            );\n\n            // Eyes lead the head slightly\n            RotateEyes(targetDirection);\n        }\n\n        // Periodic blinking for realism\n        if (Time.time > nextBlinkTime)\n        {\n            Blink();\n            nextBlinkTime = Time.time + Random.Range(2f, 6f);\n        }\n\n        // Gaze shifting (humans don't stare continuously)\n        gazeShiftTimer += Time.deltaTime;\n        if (gazeShiftTimer > Random.Range(3f, 8f))\n        {\n            ShiftGaze();\n            gazeShiftTimer = 0f;\n        }\n    }\n\n    void RotateEyes(Vector3 direction)\n    {\n        // Eyes can rotate independently of head (up to ~35 degrees)\n        Quaternion eyeRotation = Quaternion.LookRotation(direction);\n        leftEye.rotation = Quaternion.Slerp(leftEye.rotation, eyeRotation, Time.deltaTime * 5f);\n        rightEye.rotation = Quaternion.Slerp(rightEye.rotation, eyeRotation, Time.deltaTime * 5f);\n    }\n\n    void Blink()\n    {\n        // Animate eyelid closure (requires blend shapes)\n        // Typical blink duration: 100-150ms\n        StartCoroutine(BlinkAnimation());\n    }\n\n    System.Collections.IEnumerator BlinkAnimation()\n    {\n        float blinkDuration = 0.15f;\n        float elapsed = 0f;\n\n        while (elapsed < blinkDuration)\n        {\n            float blinkAmount = Mathf.Sin((elapsed / blinkDuration) * Mathf.PI);\n            // Apply to eyelid blend shape\n            elapsed += Time.deltaTime;\n            yield return null;\n        }\n    }\n\n    void ShiftGaze()\n    {\n        // Humans periodically look away during conversation\n        // Shift gaze to nearby point, then return\n        Vector3 randomOffset = Random.insideUnitSphere * 0.5f;\n        currentTarget.position += randomOffset;\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h3,{id:"proxemics-respecting-personal-space",children:"Proxemics: Respecting Personal Space"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Edward T. Hall's Proxemic Zones:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Intimate (0-0.5m)"}),": Close relationships only"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Personal (0.5-1.2m)"}),": Friends and family"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Social (1.2-3.6m)"}),": Acquaintances and strangers"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Public (3.6m+)"}),": Formal interactions"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"public class ProxemicsController : MonoBehaviour\n{\n    public Transform humanTarget;\n    public float intimateDistance = 0.5f;\n    public float personalDistance = 1.2f;\n    public float socialDistance = 3.6f;\n\n    public enum ProxemicZone { Intimate, Personal, Social, Public }\n\n    public ProxemicZone GetCurrentZone()\n    {\n        float distance = Vector3.Distance(transform.position, humanTarget.position);\n\n        if (distance < intimateDistance) return ProxemicZone.Intimate;\n        if (distance < personalDistance) return ProxemicZone.Personal;\n        if (distance < socialDistance) return ProxemicZone.Social;\n        return ProxemicZone.Public;\n    }\n\n    void Update()\n    {\n        ProxemicZone zone = GetCurrentZone();\n\n        // Adjust robot behavior based on zone\n        switch (zone)\n        {\n            case ProxemicZone.Intimate:\n                // Reduce movement speed, softer voice\n                GetComponent<RobotController>().maxSpeed = 0.3f;\n                GetComponent<AudioSource>().volume = 0.5f;\n                break;\n\n            case ProxemicZone.Personal:\n                // Normal interaction\n                GetComponent<RobotController>().maxSpeed = 0.8f;\n                GetComponent<AudioSource>().volume = 0.7f;\n                break;\n\n            case ProxemicZone.Social:\n                // Louder voice, more expressive gestures\n                GetComponent<RobotController>().maxSpeed = 1.0f;\n                GetComponent<AudioSource>().volume = 0.9f;\n                break;\n\n            case ProxemicZone.Public:\n                // Formal, reduced interaction\n                GetComponent<RobotController>().maxSpeed = 1.0f;\n                GetComponent<AudioSource>().volume = 1.0f;\n                break;\n        }\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"gesture-recognition-and-generation",children:"Gesture Recognition and Generation"}),"\n",(0,r.jsx)(e.h3,{id:"recognizing-human-gestures",children:"Recognizing Human Gestures"}),"\n",(0,r.jsx)(e.p,{children:"Unity's input system combined with computer vision can detect human gestures."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class GestureRecognizer : MonoBehaviour\n{\n    public Transform rightHand;\n    public Transform leftHand;\n\n    private List<Vector3> rightHandTrajectory = new List<Vector3>();\n    private List<Vector3> leftHandTrajectory = new List<Vector3>();\n\n    public enum Gesture { Wave, Point, Thumbsup, Stop, Come, None }\n\n    void Update()\n    {\n        // Record hand positions\n        rightHandTrajectory.Add(rightHand.position);\n        leftHandTrajectory.Add(leftHand.position);\n\n        // Keep only recent history (2 seconds at 60 FPS = 120 frames)\n        if (rightHandTrajectory.Count > 120)\n        {\n            rightHandTrajectory.RemoveAt(0);\n            leftHandTrajectory.RemoveAt(0);\n        }\n\n        // Analyze trajectory for gestures\n        Gesture detected = AnalyzeGesture(rightHandTrajectory);\n\n        if (detected != Gesture.None)\n        {\n            OnGestureDetected(detected);\n        }\n    }\n\n    Gesture AnalyzeGesture(List<Vector3> trajectory)\n    {\n        if (trajectory.Count < 30) return Gesture.None;\n\n        // Wave detection: lateral oscillation\n        if (IsWave(trajectory))\n            return Gesture.Wave;\n\n        // Point detection: extended arm with stable position\n        if (IsPointing(trajectory))\n            return Gesture.Point;\n\n        // Stop gesture: palm facing forward\n        if (IsStopGesture())\n            return Gesture.Stop;\n\n        return Gesture.None;\n    }\n\n    bool IsWave(List<Vector3> trajectory)\n    {\n        // Detect side-to-side motion\n        float lateralVariance = CalculateLateralVariance(trajectory);\n        return lateralVariance > 0.3f; // Threshold for wave motion\n    }\n\n    bool IsPointing(List<Vector3> trajectory)\n    {\n        // Check if hand is extended and stable\n        Vector3 recent = trajectory[trajectory.Count - 1];\n        Vector3 shoulderPos = transform.position + Vector3.up * 1.5f;\n\n        float armExtension = Vector3.Distance(recent, shoulderPos);\n        float stability = CalculateStability(trajectory);\n\n        return armExtension > 0.6f && stability < 0.1f;\n    }\n\n    bool IsStopGesture()\n    {\n        // Check palm orientation (requires hand tracking data)\n        // Simplified: check if hand is at chest height with outward facing\n        return rightHand.position.y > transform.position.y + 1.0f;\n    }\n\n    float CalculateLateralVariance(List<Vector3> trajectory)\n    {\n        float sum = 0f;\n        for (int i = 1; i < trajectory.Count; i++)\n        {\n            sum += Mathf.Abs(trajectory[i].x - trajectory[i-1].x);\n        }\n        return sum / trajectory.Count;\n    }\n\n    float CalculateStability(List<Vector3> trajectory)\n    {\n        Vector3 mean = Vector3.zero;\n        foreach (Vector3 pos in trajectory)\n            mean += pos;\n        mean /= trajectory.Count;\n\n        float variance = 0f;\n        foreach (Vector3 pos in trajectory)\n            variance += Vector3.Distance(pos, mean);\n\n        return variance / trajectory.Count;\n    }\n\n    void OnGestureDetected(Gesture gesture)\n    {\n        Debug.Log($"Detected gesture: {gesture}");\n\n        // Respond to gesture\n        switch (gesture)\n        {\n            case Gesture.Wave:\n                RespondWithWave();\n                break;\n            case Gesture.Come:\n                MoveTowardsPerson();\n                break;\n            case Gesture.Stop:\n                Halt();\n                break;\n        }\n    }\n\n    void RespondWithWave()\n    {\n        // Trigger wave animation\n        GetComponent<Animator>().SetTrigger("Wave");\n    }\n\n    void MoveTowardsPerson()\n    {\n        // Navigate to human position\n        GetComponent<UnityEngine.AI.NavMeshAgent>().SetDestination(humanTarget.position);\n    }\n\n    void Halt()\n    {\n        GetComponent<UnityEngine.AI.NavMeshAgent>().isStopped = true;\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"generating-expressive-robot-gestures",children:"Generating Expressive Robot Gestures"}),"\n",(0,r.jsx)(e.p,{children:"Robots must generate natural, meaningful gestures."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'public class GestureGenerator : MonoBehaviour\n{\n    public Animator animator;\n\n    public void ExpressEmotion(string emotion)\n    {\n        switch (emotion.ToLower())\n        {\n            case "happy":\n                animator.SetTrigger("Celebrate");\n                // Open arms, upward gaze\n                break;\n\n            case "confused":\n                animator.SetTrigger("HeadTilt");\n                animator.SetTrigger("ShrugShoulders");\n                break;\n\n            case "agreement":\n                animator.SetTrigger("Nod");\n                break;\n\n            case "disagreement":\n                animator.SetTrigger("Shake");\n                break;\n        }\n    }\n\n    public void PointAt(Vector3 worldPosition)\n    {\n        // Calculate which arm to use based on position\n        bool useRightArm = worldPosition.x > transform.position.x;\n\n        // Set IK target for pointing\n        if (useRightArm)\n        {\n            animator.SetIKPositionWeight(AvatarIKGoal.RightHand, 1f);\n            animator.SetIKPosition(AvatarIKGoal.RightHand, worldPosition);\n        }\n        else\n        {\n            animator.SetIKPositionWeight(AvatarIKGoal.LeftHand, 1f);\n            animator.SetIKPosition(AvatarIKGoal.LeftHand, worldPosition);\n        }\n\n        // Look at target\n        animator.SetLookAtWeight(1f);\n        animator.SetLookAtPosition(worldPosition);\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"natural-language-interaction",children:"Natural Language Interaction"}),"\n",(0,r.jsx)(e.h3,{id:"speech-synthesis-integration",children:"Speech Synthesis Integration"}),"\n",(0,r.jsx)(e.p,{children:"Unity can integrate with text-to-speech engines for natural voice output."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.Networking;\nusing System.Collections;\n\npublic class RobotSpeech : MonoBehaviour\n{\n    public AudioSource audioSource;\n    private string ttsApiUrl = "https://api.openai.com/v1/audio/speech";\n\n    public IEnumerator Speak(string text)\n    {\n        // Generate speech using OpenAI TTS\n        UnityWebRequest request = UnityWebRequest.Post(ttsApiUrl, "");\n\n        string jsonBody = $@"{{\n            ""model"": ""tts-1"",\n            ""voice"": ""nova"",\n            ""input"": ""{text}""\n        }}";\n\n        byte[] bodyRaw = System.Text.Encoding.UTF8.GetBytes(jsonBody);\n        request.uploadHandler = new UploadHandlerRaw(bodyRaw);\n        request.downloadHandler = new DownloadHandlerAudioClip(ttsApiUrl, AudioType.MPEG);\n        request.SetRequestHeader("Content-Type", "application/json");\n        request.SetRequestHeader("Authorization", "Bearer " + GetApiKey());\n\n        yield return request.SendWebRequest();\n\n        if (request.result == UnityWebRequest.Result.Success)\n        {\n            AudioClip clip = DownloadHandlerAudioClip.GetContent(request);\n            audioSource.clip = clip;\n            audioSource.Play();\n\n            // Animate mouth while speaking\n            StartCoroutine(AnimateMouth(clip.length));\n        }\n    }\n\n    IEnumerator AnimateMouth(float duration)\n    {\n        float elapsed = 0f;\n        Animator animator = GetComponent<Animator>();\n\n        while (elapsed < duration)\n        {\n            // Simulate lip movement\n            float mouthOpen = Mathf.PerlinNoise(Time.time * 10f, 0f);\n            animator.SetFloat("MouthOpen", mouthOpen);\n\n            elapsed += Time.deltaTime;\n            yield return null;\n        }\n\n        animator.SetFloat("MouthOpen", 0f);\n    }\n\n    string GetApiKey()\n    {\n        return System.Environment.GetEnvironmentVariable("OPENAI_API_KEY");\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"vrar-integration-for-human-robot-testing",children:"VR/AR Integration for Human-Robot Testing"}),"\n",(0,r.jsx)(e.h3,{id:"testing-hri-in-virtual-reality",children:"Testing HRI in Virtual Reality"}),"\n",(0,r.jsx)(e.p,{children:"VR allows humans to interact with simulated robots before hardware exists."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.XR;\n\npublic class VRHRITester : MonoBehaviour\n{\n    public Transform vrLeftHand;\n    public Transform vrRightHand;\n    public Transform humanoidRobot;\n\n    void Update()\n    {\n        // Detect hand gestures in VR\n        if (IsGrabbingGesture(XRNode.RightHand))\n        {\n            TryGrabObject();\n        }\n\n        // Track personal space violations\n        float distance = Vector3.Distance(vrLeftHand.position, humanoidRobot.position);\n        if (distance < 0.5f)\n        {\n            humanoidRobot.GetComponent<ProxemicsController>().OnPersonalSpaceViolation();\n        }\n    }\n\n    bool IsGrabbingGesture(XRNode hand)\n    {\n        InputDevice device = InputDevices.GetDeviceAtXRNode(hand);\n        device.TryGetFeatureValue(CommonUsages.grip, out float gripValue);\n        return gripValue > 0.8f;\n    }\n\n    void TryGrabObject()\n    {\n        // Raycast from hand to detect objects\n        RaycastHit hit;\n        if (Physics.Raycast(vrRightHand.position, vrRightHand.forward, out hit, 0.5f))\n        {\n            if (hit.collider.CompareTag("Grabbable"))\n            {\n                // Attach object to hand\n                hit.transform.SetParent(vrRightHand);\n            }\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"performance-optimization-for-real-time-interaction",children:"Performance Optimization for Real-Time Interaction"}),"\n",(0,r.jsx)(e.h3,{id:"rendering-optimizations",children:"Rendering Optimizations"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"public class RenderingOptimizer : MonoBehaviour\n{\n    void Start()\n    {\n        // Use LOD (Level of Detail) for distant robots\n        LODGroup lodGroup = gameObject.AddComponent<LODGroup>();\n\n        LOD[] lods = new LOD[3];\n\n        // High detail (0-10m)\n        lods[0] = new LOD(0.1f, GetComponent<Renderer>().materials);\n\n        // Medium detail (10-30m)\n        lods[1] = new LOD(0.03f, GetLowerPolyRenderers());\n\n        // Low detail (30m+)\n        lods[2] = new LOD(0.01f, GetLowestPolyRenderers());\n\n        lodGroup.SetLODs(lods);\n        lodGroup.RecalculateBounds();\n\n        // Enable GPU instancing for repeated objects\n        foreach (Renderer r in GetComponentsInChildren<Renderer>())\n        {\n            r.material.enableInstancing = true;\n        }\n\n        // Occlusion culling for indoor environments\n        Camera.main.useOcclusionCulling = true;\n    }\n\n    Renderer[] GetLowerPolyRenderers()\n    {\n        // Return simplified mesh renderers\n        return new Renderer[0]; // Placeholder\n    }\n\n    Renderer[] GetLowestPolyRenderers()\n    {\n        return new Renderer[0]; // Placeholder\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"High-fidelity rendering in Unity enables:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Photorealistic training environments"})," for computer vision systems"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Natural human-robot interaction"})," through gaze, gestures, and proxemics"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Immersive VR/AR testing"})," before deploying physical robots"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Expressive robot behaviors"})," that build trust and understanding"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Key Takeaways:"})}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Use HDRP for physically-accurate materials and lighting"}),"\n",(0,r.jsx)(e.li,{children:"Implement natural gaze and gesture systems"}),"\n",(0,r.jsx)(e.li,{children:"Respect human proxemic zones for comfortable interaction"}),"\n",(0,r.jsx)(e.li,{children:"Integrate speech synthesis for natural communication"}),"\n",(0,r.jsx)(e.li,{children:"Optimize rendering for real-time performance"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"By mastering these techniques, you can create digital twins that not only look realistic but behave in ways that feel natural and safe to humans\u2014essential for the future of humanoid robotics."})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>s});var t=i(6540);const r={},o=t.createContext(r);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);